<!DOCTYPE html>
<html lang="fa" dir="rtl">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>بررسی زیرساخت ابرمقیاس متا | mhhio&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="شرکت های ابرمقیاس (Hyperscalers) مانند علی‌بابا، آمازون، بایت‌دنس، گوگل، متا، مایکروسافت و تنسنت، زیرساخت‌های در مقیاس کره‌زمین توسعه داده‌اند تا خدمات ابری، وب یا موبایل را به کاربران جهانی خود ارائه دهند. اگرچه ممکن است بیشتر متخصصان به طور مستقیم چنین زیرساخت‌های هایپرسکیل را نسازند، معتقدیم که یادگیری درباره‌ی آن‌ها مفید است. از نظر تاریخی، بسیاری از فناوری‌های پرکاربرد از محیط‌های پیشرفته سرچشمه گرفته‌اند، از جمله مین‌فریم‌ها (Mainframes) در دهه‌ی 1960 و زیرساخت‌های هایپرسکیل در دو دهه‌ی اخیر.">
<meta name="author" content="mhhio">
<link rel="canonical" href="https://mhhio.github.io/posts/metas-hyperscale-infrastructure-overview-and-insights/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.0c4fd3725171366f335155acde6fb3c7b7042cd2fd075bebf5023d9b28a701b2.css" integrity="sha256-DE/TclFxNm8zUVWs3m&#43;zx7cELNL9B1vr9QI9myinAbI=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://mhhio.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mhhio.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mhhio.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mhhio.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://mhhio.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Vazirmatn:wght@100;200;300;400;500;600;700;800;900&display=swap" rel="stylesheet">

<style>
    :root {
        --main-font: 'Vazirmatn', sans-serif;
    }

    body {
        font-family: var(--main-font) !important;
    }

    .post-title {
        font-family: var(--main-font) !important;
    }

    .post-content {
        font-family: var(--main-font) !important;
    }

    .post-meta {
        font-family: var(--main-font) !important;
    }

    .entry-header {
        font-family: var(--main-font) !important;
    }

    .entry-content {
        font-family: var(--main-font) !important;
    }

     
    [dir="rtl"] body {
        font-family: var(--main-font) !important;
    }

     
    .post-content figure figcaption {
        text-align: center !important;
         
        margin-top: 10px;
        font-style: italic;
    }

     
    .post-content figure {
        text-align: center;
        margin: 1.5em auto;
    }
</style><meta property="og:title" content="بررسی زیرساخت ابرمقیاس متا" />
<meta property="og:description" content="شرکت های ابرمقیاس (Hyperscalers) مانند علی‌بابا، آمازون، بایت‌دنس، گوگل، متا، مایکروسافت و تنسنت، زیرساخت‌های در مقیاس کره‌زمین توسعه داده‌اند تا خدمات ابری، وب یا موبایل را به کاربران جهانی خود ارائه دهند. اگرچه ممکن است بیشتر متخصصان به طور مستقیم چنین زیرساخت‌های هایپرسکیل را نسازند، معتقدیم که یادگیری درباره‌ی آن‌ها مفید است. از نظر تاریخی، بسیاری از فناوری‌های پرکاربرد از محیط‌های پیشرفته سرچشمه گرفته‌اند، از جمله مین‌فریم‌ها (Mainframes) در دهه‌ی 1960 و زیرساخت‌های هایپرسکیل در دو دهه‌ی اخیر." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mhhio.github.io/posts/metas-hyperscale-infrastructure-overview-and-insights/" /><meta property="og:image" content="https://mhhio.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-02-16T20:53:59+03:30" />
<meta property="article:modified_time" content="2025-02-16T20:53:59+03:30" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mhhio.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="بررسی زیرساخت ابرمقیاس متا"/>
<meta name="twitter:description" content="شرکت های ابرمقیاس (Hyperscalers) مانند علی‌بابا، آمازون، بایت‌دنس، گوگل، متا، مایکروسافت و تنسنت، زیرساخت‌های در مقیاس کره‌زمین توسعه داده‌اند تا خدمات ابری، وب یا موبایل را به کاربران جهانی خود ارائه دهند. اگرچه ممکن است بیشتر متخصصان به طور مستقیم چنین زیرساخت‌های هایپرسکیل را نسازند، معتقدیم که یادگیری درباره‌ی آن‌ها مفید است. از نظر تاریخی، بسیاری از فناوری‌های پرکاربرد از محیط‌های پیشرفته سرچشمه گرفته‌اند، از جمله مین‌فریم‌ها (Mainframes) در دهه‌ی 1960 و زیرساخت‌های هایپرسکیل در دو دهه‌ی اخیر."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://mhhio.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "بررسی زیرساخت ابرمقیاس متا",
      "item": "https://mhhio.github.io/posts/metas-hyperscale-infrastructure-overview-and-insights/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "بررسی زیرساخت ابرمقیاس متا",
  "name": "بررسی زیرساخت ابرمقیاس متا",
  "description": "شرکت های ابرمقیاس (Hyperscalers) مانند علی‌بابا، آمازون، بایت‌دنس، گوگل، متا، مایکروسافت و تنسنت، زیرساخت‌های در مقیاس کره‌زمین توسعه داده‌اند تا خدمات ابری، وب یا موبایل را به کاربران جهانی خود ارائه دهند. اگرچه ممکن است بیشتر متخصصان به طور مستقیم چنین زیرساخت‌های هایپرسکیل را نسازند، معتقدیم که یادگیری درباره‌ی آن‌ها مفید است. از نظر تاریخی، بسیاری از فناوری‌های پرکاربرد از محیط‌های پیشرفته سرچشمه گرفته‌اند، از جمله مین‌فریم‌ها (Mainframes) در دهه‌ی 1960 و زیرساخت‌های هایپرسکیل در دو دهه‌ی اخیر.",
  "keywords": [
    
  ],
  "articleBody": "شرکت های ابرمقیاس (Hyperscalers) مانند علی‌بابا، آمازون، بایت‌دنس، گوگل، متا، مایکروسافت و تنسنت، زیرساخت‌های در مقیاس کره‌زمین توسعه داده‌اند تا خدمات ابری، وب یا موبایل را به کاربران جهانی خود ارائه دهند. اگرچه ممکن است بیشتر متخصصان به طور مستقیم چنین زیرساخت‌های هایپرسکیل را نسازند، معتقدیم که یادگیری درباره‌ی آن‌ها مفید است. از نظر تاریخی، بسیاری از فناوری‌های پرکاربرد از محیط‌های پیشرفته سرچشمه گرفته‌اند، از جمله مین‌فریم‌ها (Mainframes) در دهه‌ی 1960 و زیرساخت‌های هایپرسکیل در دو دهه‌ی اخیر. به عنوان مثال، حافظه‌ی مجازی (Virtual Memory) ابتدا در مین‌فریم‌ها به وجود آمد و اکنون حتی در ساعت‌های هوشمند نیز رایج است. به طور مشابه، Kubernetes و PyTorch به ترتیب در گوگل و فیسبوک ایجاد شدند، اما توسط سازمان‌هایی با اندازه‌های مختلف مورد استفاده قرار گرفته‌اند. علاوه بر این فناوری‌های خاص، اصول و درس‌های آموخته شده از زیرساخت‌های هایپرسکیل ممکن است به متخصصان کمک کند تا به طور کلی سیستم‌های بهتری بسازند.\nاین مقاله مروری کلی بر زیرساخت‌های هایپرسکیل متا ارائه می‌دهد و بر بینش‌های کلیدی حاصل از توسعه‌ی آن، به ویژه در نرم‌افزارهای سیستمی، تمرکز دارد. در موارد مرتبط، تفاوت‌های آن با ابرهای عمومی (Public Clouds) را برجسته می‌کنیم، زیرا محدودیت‌های مختلف منجر به بهینه‌سازی‌های متمایزی شده‌اند. اگرچه بسیاری از دانش ارائه‌شده در اینجا قبلاً در صنعت و جامعه‌ی تحقیقاتی به اشتراک گذاشته شده و به کار گرفته شده است، اما سهم اصلی این مقاله ارائه‌ی یک دیدگاه جامع است که به خوانندگان کمک می‌کند تا یک مدل ذهنی کامل از زیرساخت‌های هایپرسکیل را از ابتدا تا انتها درک کنند.\nفرهنگ مهندسی قبل از پرداختن به جزئیات زیرساخت‌های متا، ابتدا به چند جنبه از فرهنگ مهندسی این شرکت اشاره می‌کنیم، زیرا فرهنگ یک سازمان تأثیر زیادی بر فناوری آن دارد.\nسرعت بالا از زمان تأسیس، فیسبوک فرهنگ “سرعت بالا” را در خود نهادینه کرده و حفظ کرده است، فرهنگی که بر چابکی و تکرار سریع تأکید دارد. این فلسفه در تعهد قوی شرکت به استقرار مداوم نرم‌افزار (Continuous Software Deployment) مشهود است، جایی که آخرین تغییرات کد در اسرع وقت به محیط عملیاتی (Production) منتقل می‌شود. علاوه بر این، مهندسان محصول عمدتاً کدهای خود را در قالب توابع بدون حالت (Stateless) و سرورلس (Serverless) به زبان‌هایی مانند PHP، Python و Erlang می‌نویسند، زیرا این رویکرد سادگی، بهره‌وری و سرعت توسعه و به‌روزرسانی را افزایش می‌دهد. تیم‌ها این امکان را دارند که به‌سرعت اولویت‌های اجرایی خود را تغییر دهند، بدون آنکه نیاز به یک فرایند برنامه‌ریزی طولانی داشته باشند، و می‌توانند مسائل مبهم را در حین اجرای تدریجی حل کنند. این رویکرد به آن‌ها اجازه می‌دهد تا به‌سرعت با شرایط متغیر بازار سازگار شده و محصولات جدیدی را عرضه کنند.\nباز بودن فناوری متا از باز بودن فناوری، هم به صورت داخلی و هم خارجی، حمایت می‌کند. در داخل شرکت، ما از رویکرد مونورپو (Monorepo) استفاده می‌کنیم، جایی که کد تمام پروژه‌ها در یک مخزن واحد ذخیره می‌شود تا یافتن و استفاده مجدد از کد و همچنین مشارکت بین تیم‌ها تسهیل شود. در حالی که سازمان‌های دیگر نیز از مونورپو استفاده می‌کنند، میزان باز بودن آن‌ها در استفاده از این رویکرد متفاوت است. برخی از سازمان‌ها برای هر پروژه مالکانی تعیین می‌کنند و تنها این مالکان مجاز به پذیرش تغییرات کد هستند، اگرچه دیگران می‌توانند تغییرات را پیشنهاد دهند. در مقابل، با چند استثنا، اکثر پروژه‌ها در متا چنین قوانین سختگیرانه‌ای در مورد مالکیت اعمال نمی‌کنند. این باز بودن، مشارکت بین تیم‌ها و استفاده مجدد از کد را تشویق می‌کند و از نوشتن مجدد کدهای مشابه جلوگیری می‌کند.\nدر متا، مهندسان تغییرات کد را مستقیماً به خط اصلی (Mainline) مونورپو کامیت می‌کنند، و استقرار نرم‌افزارها از خط اصلی، یعنی از آخرین نسخه کد، کامپایل می‌شود، برخلاف برخی شاخه‌های پایدار. به عنوان مثال، هنگامی که یک کتابخانه پراستفاده، مانند کتابخانه RPC، به‌روزرسانی می‌شود، نسخه بعدی هر برنامه‌ای که به این کتابخانه وابسته است، به طور خودکار با آخرین نسخه آن کامپایل می‌شود.\nدر خارج از شرکت، تعهد متا به باز بودن فناوری از طریق طراحی‌های سخت‌افزاری متن‌باز در پروژه Open Compute و پروژه‌های نرم‌افزاری مانند PyTorch، Llama، Presto، RocksDB و Cassandra نشان داده می‌شود. همچنین، بخش زیادی از فناوری زیرساخت متا از طریق مقالات تحقیقاتی به اشتراک گذاشته شده است، که بسیاری از آن‌ها در مراجع این مقاله ذکر شده‌اند.\nتحقیق در محیط عملیاتی زیرساخت‌های هایپرسکیل متا نیازمند نوآوری مداوم است، اما برخلاف بیشتر هایپرسکیلرها، این شرکت یک آزمایشگاه تحقیقاتی سیستم‌های اختصاصی ندارد. در عوض، تمام مقالات تحقیقاتی سیستم‌های آن توسط تیم‌هایی نوشته می‌شود که سیستم‌های عملیاتی را توسعه می‌دهند. این تیم‌ها در حالی که مسائل چالش‌برانگیز عملیاتی را در مقیاس بزرگ حل می‌کنند، مرزهای فناوری را پیش می‌برند و سپس این تجربیات را در قالب مقالات تحقیقاتی منتشر می‌کنند. این رویکرد تضمین می‌کند که مشکلات مطرح شده واقعی هستند و راه‌حل‌ها در مقیاس بزرگ کار می‌کنند، که با معیارهای کلیدی موفقیت در تحقیقات سیستم‌ها هم‌خوانی دارد.\nزیرساخت مشترک در حالی که برخی سازمان‌ها به تیم‌های فردی اجازه می‌دهند تا در مورد پشته فناوری خود تصمیم‌گیری محلی داشته باشند، متا استانداردسازی و بهینه‌سازی جهانی را در اولویت قرار می‌دهد. در بخش سخت‌افزار، سرورهایی که از محصولات مختلف پشتیبانی می‌کنند، همگی از یک استخر سرور مشترک تخصیص داده می‌شوند. علاوه بر این، برای بارهای کاری غیرهوش مصنوعی (Non-AI Compute Workloads)، ما تنها یک نوع سرور ارائه می‌دهیم که مجهز به یک CPU و مقدار مشخصی از DRAM (قبلاً 64 گیگابایت، اکنون 256 گیگابایت) است. برخلاف ابرهای عمومی که باید انواع مختلف سرورها را برای تطبیق با برنامه‌های متنوع مشتریان ارائه دهند، متا می‌تواند برنامه‌های خود را برای تطبیق با سخت‌افزار بهینه‌سازی کند، و در نتیجه از افزایش انواع سرورها جلوگیری کند.\nاستانداردسازی در بخش نرم‌افزار نیز حاکم است. به عنوان مثال، محصولات مختلف متا قبلاً از Cassandra، HBase و ZippyDB برای ذخیره‌سازی کلید-مقدار (Key-Value Store) استفاده می‌کردند، اما اکنون همگی به ZippyDB روی آورده‌اند. علاوه بر این، هر قابلیت مشترک—مانند استقرار نرم‌افزار، مدیریت پیکربندی، شبکه سرویس (Service Mesh)، تست عملکرد قبل از تولید، نظارت بر عملکرد در محیط عملیاتی، و تست بار در محیط عملیاتی—توسط یک ابزار جهانی که به طور گسترده پذیرفته شده است، پشتیبانی می‌شود.\nعلاوه بر استانداردسازی، یک اصل کلیدی در دستیابی به زیرساخت مشترک، ترجیح ما برای استفاده از اجزای قابل استفاده مجدد به جای راه‌حل‌های یکپارچه (Monolithic) است. یک مثال خوب از این موضوع، زنجیره استفاده مجدد از اجزا در سیستم فایل توزیع‌شده ما به نام Tectonic است. Tectonic با استفاده از یک ذخیره‌سازی کلید-مقدار توزیع‌شده به نام ZippyDB، مقیاس‌پذیری خود را افزایش می‌دهد. ZippyDB نیز به نوبه خود از یک چارچوب مشترک برای مدیریت قطعات داده (Sharding) به نام Shard Manager استفاده می‌کند. Shard Manager نیز برای کشف قطعات و مسیریابی درخواست‌ها به ServiceRouter متا وابسته است. در نهایت، ServiceRouter داده‌های کشف سرویس و پیکربندی کل زیرساخت را در یک فضای ذخیره‌سازی داده بسیار قابل اعتماد و بدون وابستگی به نام Delos ذخیره می‌کند. بنابراین، زنجیره استفاده مجدد از اجزا به این شکل است: Tectonic → ZippyDB → Shard Manager → ServiceRouter → Delos. تمام این اجزای قابل استفاده مجدد، برای بسیاری از موارد استفاده دیگر نیز به کار می‌روند. در مقابل، HDFS، یکی از محبوب‌ترین سیستم‌های فایل توزیع‌شده متن‌باز، به‌صورت یکپارچه(monolithic) طراحی شده و تمامی این مؤلفه‌ها را درون خود مدیریت می‌کند.[که می‌تواند انعطاف‌پذیری و مقیاس‌پذیری را محدود کند.]\nمطالعه موردی فرهنگ: برنامه Threads توسعه برنامه Threads، که اغلب با توییتر/X مقایسه می‌شود، نمونه‌ای از فرهنگ ذکر شده است. با تأکید بر حرکت سریع، یک تیم کوچک Threads را تنها در پنج ماه کار فنی در محیطی شبیه به استارت‌آپ توسعه داد. علاوه بر این، پس از توسعه، تیم‌های زیرساخت تنها دو روز فرصت داشتند تا برای راه‌اندازی آن در محیط عملیاتی آماده شوند. بیشتر سازمان‌های بزرگ بیش از دو روز زمان صرف می‌کنند تا فقط [طرح] یک برنامه‌کاربردی را که شامل ده‌ها تیم وابسته به هم است را تدوین کنند، چه برسد به اجرای آن. اما در متا، ما به سرعت اتاق‌های جنگ (War Rooms) را در سایت‌های توزیع‌شده ایجاد کردیم و تیم‌های زیرساخت و محصول را گرد هم آوردیم تا مسائل را به صورت بلادرنگ حل‌وفصل کنند. با وجود زمان‌بندی فشرده، راه‌اندازی این برنامه بسیار موفقیت‌آمیز بود و تنها در پنج روز به 100 میلیون کاربر رسید، که آن را به سریع‌ترین برنامه در حال رشد در تاریخ تبدیل کرد.\nزیرساخت مشترک نقش کلیدی در توانایی تیم‌ها برای پیاده‌سازی سریع Threads و مقیاس‌پذیری قابل اعتماد آن داشت. Threads از بک‌اند پایتون اینستاگرام و همچنین اجزای زیرساخت مشترک متا، مانند پایگاه داده گراف اجتماعی، ذخیره‌سازی کلید-مقدار، پلتفرم سرورلس، پلتفرم‌های آموزش و استنتاج یادگیری ماشین (ML)، و چارچوب مدیریت پیکربندی برای برنامه‌های موبایل، استفاده مجدد کرد.\nباز بودن فناوری داخلی متا، با استفاده از مونوریپو، به Threads اجازه داد تا از برخی کدهای برنامه اینستاگرام استفاده مجدد کند و توسعه خود را تسریع بخشد. در مورد باز بودن فناوری خارجی، Threads قصد دارد با ActivityPub، پروتکل شبکه‌سازی اجتماعی متن‌باز، ادغام شود تا با سایر برنامه‌ها سازگاری داشته باشد. ما همچنین تجربیات خود را از توسعه سریع Threads به صورت عمومی به اشتراک گذاشته‌ایم.\nجریان درخواست کاربر از ابتدا تا انتها اکنون به بررسی فناوری زیرساخت متا می‌پردازیم. محصولات متا توسط یک زیرساخت خدمات مشترک پشتیبانی می‌شوند. برای ارائه یک دیدگاه جامع از این زیرساخت، توضیح می‌دهیم که چگونه یک درخواست کاربر از ابتدا تا انتها پردازش می‌شود و تمام اجزای درگیر را به تفصیل شرح می‌دهیم.\nمسیریابی درخواست نگاشت دینامیک DNS هنگامی که کاربر درخواستی به facebook.com ارسال می‌کند، سرور DNS متا به صورت دینامیک یک آدرس IP برمی‌گرداند که به یک مرکز داده کوچک لبه (Edge Datacenter) که توسط متا اداره می‌شود و به عنوان نقطه حضور (Point of Presence یا PoP) شناخته می‌شود، نگاشت شده است (همانطور که در شکل ۱ نشان داده شده است). این نگاشت دینامیک DNS اطمینان می‌دهد که PoP انتخاب شده به کاربر نزدیک باشد و در عین حال بار را بین PoPها متعادل کند. اتصال TCP کاربر در PoP خاتمه می‌یابد، که اتصالات TCP جداگانه و طولانی‌مدتی با مراکز داده متا حفظ می‌کند. این تنظیم TCP تقسیم‌شده (Split-TCP) مزایای متعددی دارد، از جمله کاهش تأخیر ایجاد TCP از طریق استفاده مجدد از اتصالات از پیش برقرار شده بین PoPها و مراکز داده. یک PoP معمولاً صدها سرور دارد اما ممکن است تا چند هزار سرور نیز داشته باشد. صدها PoP در سراسر جهان قرار گرفته‌اند تا اطمینان حاصل شود که بیشتر کاربران یک PoP نزدیک به خود دارند، و در نتیجه تأخیر شبکه کوتاه‌مدت را تضمین کنند.\nکش محتوای استاتیک اگر درخواست کاربر برای محتوای استاتیک باشد، مانند تصاویر و ویدیوها، در صورتی که محتوا از قبل در PoP کش شده باشد، می‌تواند مستقیماً از PoP ارائه شود. علاوه بر این، محتوای استاتیک ممکن است توسط شبکه تحویل محتوا (CDN) نیز کش شود، همانطور که در شکل ۱ نشان داده شده است. هنگامی که حجم قابل توجهی از ترافیک محصولات متا از شبکه یک ارائه‌دهنده خدمات اینترنتی (ISP) سرچشمه می‌گیرد، متا به دنبال ایجاد یک مشارکت دوجانبه سودمند با ارائه تجهیزات شبکه متا (Meta Network Appliances) است که در شبکه ISP میزبانی می‌شوند تا محتوای استاتیک را کش کنند، و در نتیجه یک سایت CDN تشکیل دهند. یک سایت CDN معمولاً ده‌ها سرور دارد و برخی از آن‌ها بیش از صد سرور دارند. هزاران سایت CDN در سراسر جهان، شبکه CDN ما را برای توزیع محتوای استاتیک تشکیل می‌دهند.\nمحصولات متا از بازنویسی URL برای هدایت درخواست‌های کاربر به یک سایت CDN نزدیک استفاده می‌کنند. هنگامی که یک محصول متا یک URL برای دسترسی کاربر به محتوای استاتیک ارائه می‌دهد، URL را بازنویسی می‌کند، به عنوان مثال از facebook.com/image.jpg به CDN109.meta.com/image.jpg. اگر تصویر در CDN109 کش نشده باشد و کاربر آن را درخواست کند، CDN109 درخواست را به یک PoP نزدیک ارسال می‌کند. سپس PoP درخواست را به متعادل‌کننده بار (Load Balancer) در یک منطقه مرکز داده ارسال می‌کند، که تصویر را از سیستم ذخیره‌سازی بازیابی می‌کند. در مسیر بازگشت، هم PoP و هم سایت CDN تصویر را برای استفاده‌های بعدی کش می‌کنند.\nمسیریابی درخواست محتواهای پویا اگر درخواست کاربر برای محتوای دینامیک مانند فید خبری باشد، PoP آن را به یک منطقه مرکز داده ارسال می‌کند. انتخاب منطقه هدف توسط یک ابزار مهندسی ترافیک هدایت می‌شود که به طور دوره‌ای توزیع بهینه ترافیک جهانی از PoPها به مراکز داده را با در نظر گرفتن عواملی مانند ظرفیت مرکز داده و تأخیر شبکه محاسبه می‌کند.\nترافیک PoP به مرکز داده از طریق شبکه گسترده خصوصی (WAN) متا منتقل می‌شود، که PoPها و مراکز داده متا را در سطح جهانی با استفاده از فیبرهای نوری به طول ده‌ها هزار مایل به هم متصل می‌کند. ترافیک شبکه داخلی بین مراکز داده و PoPهای ما به طور قابل توجهی از ترافیک خارجی بین کاربران و PoPها بیشتر است، که این امر عمدتاً به دلیل تکثیر داده‌ها در مراکز داده و تعاملات بین سرویس‌های ریز (Microservices) ما است. شبکه WAN خصوصی پهنای باند بالایی را برای خدمت‌رسانی به این ترافیک داخلی فراهم می‌کند.\nشکل۱-زیرساخت جهانی متا!\nتوپولوژی زیرساخت جدول ۱ اجزای زیرساختی که پیش‌تر ذکر شد را خلاصه می‌کند. در سطح جهانی، ده‌ها منطقه مرکز داده، صدها مرکز داده لبه (PoPها) و هزاران سایت CDN وجود دارد. هر منطقه مرکز داده شامل چندین مرکز داده است که در محدوده چند مایلی یکدیگر قرار دارند. هر مرکز داده از حداکثر یک دوجین تابلوهای اصلی سوئیچ (MSB) برای توزیع برق استفاده می‌کند، که به عنوان دامنه‌های خطای زیر-مرکز داده اصلی نیز عمل می‌کنند. خرابی یک MSB می‌تواند باعث از کار افتادن ۱۰ تا ۲۰ هزار سرور شود.\nشبکه لبه (Edge Network) یک PoP به چندین سیستم مستقل (Autonomous Systems) در اینترنت متصل است و معمولاً چندین مسیر برای دسترسی به شبکه کاربر دارد. هنگام انتخاب مسیر بین یک PoP و یک کاربر، پروتکل Border Gateway Protocol (BGP) به طور پیش‌فرض ظرفیت و عملکرد شبکه را در نظر نمی‌گیرد. با این حال، شبکه PoP این عوامل را در نظر می‌گیرد و مسیر ترجیحی خود را به یک پیشوند شبکه (Network Prefix) اعلام می‌کند.\nشبکه مرکز داده (Datacenter Network) سرورها در یک مرکز داده توسط یک شبکه داخلی مرکز داده (Datacenter Fabric) به هم متصل می‌شوند، جایی که سوئیچ‌های شبکه یک توپولوژی سه‌سطحی Clos تشکیل می‌دهند که می‌تواند به صورت افزایشی با افزودن سوئیچ‌های بیشتر در سطح بالایی مقیاس‌پذیر شود. با تعداد کافی سوئیچ‌های سطح بالا، این شبکه می‌تواند یک شبکه بدون انسداد (Non-Blocking) و بدون اشباع (Non-Oversubscribed) ارائه دهد که امکان ارتباط بین هر دو سرور با حداکثر پهنای باند NIC را فراهم می‌کند. ما در حال حرکت به سمت حذف اشباع شبکه (Network Oversubscription) در داخل یک مرکز داده هستیم.\nشبکه منطقه‌ای (Regional Network) یک تجمیع‌کننده شبکه (Fabric Aggregator) مراکز داده را در یک منطقه به هم متصل کرده و آن‌ها را به شبکه WAN خصوصی ما متصل می‌کند. تجمیع‌کننده شبکه از یک توپولوژی شبیه به درخت چاق (Fat-Tree) استفاده می‌کند که امکان افزودن سوئیچ‌های بیشتر برای افزایش پهنای باند را فراهم می‌کند. ما هدفمان این است که اشباع شبکه در یک منطقه را به طور قابل توجهی کاهش دهیم تا ارتباط بین مراکز داده درون یک منطقه به یک گلوگاه تبدیل نشود. این امر به اکثر سرویس‌ها (به جز آموزش یادگیری ماشین) اجازه می‌دهد تا در مراکز داده یک منطقه پراکنده شوند بدون اینکه نگران کاهش قابل توجه عملکرد باشند.\nجدول۱-تعداد و اندازه‌ی اجزای زیرساخت متا نوع زیرساخت تعداد تعداد سرور در هر زیرساخت منطقه‌ای O(10) یک میلیون PoP O(100) از ده ها تا هزاران سایت CDN O(1,000) ده‌ها تا بیش از صد دیتاسنتر چندین دیتاسنتر در هر منطقه صدهزاران MSB ده‌ها MSB در هر دیتاسنتر بین ۱۰ هزار تا ۲۰ هزار پردازش درخواست پردازش آنلاین هنگامی که یک درخواست کاربر به یک منطقه مرکز داده می‌رسد، در طول مسیری که در شکل ۲ نشان داده شده است، پردازش می‌شود. متعادل‌کننده بار (Load Balancer) درخواست‌های کاربر را بین ده‌ها هزار سرور توزیع می‌کند که “توابع سرورلس فرانت‌اند” را اجرا می‌کنند. برای پردازش یک درخواست کاربر، یک تابع سرورلس فرانت‌اند ممکن است بسیاری از سرویس‌های بک‌اند را فراخوانی کند، که برخی از آن‌ها ممکن است به “استنتاج یادگیری ماشین (ML Inference)” نیز متکی باشند، مثلاً برای بازیابی توصیه‌های تبلیغات یا محتوای فید خبری. در طول اجرا، یک تابع سرورلس فرانت‌اند می‌تواند رویدادهایی را در “صف رویداد (Event Queue)” قرار دهد تا “توابع سرورلس رویداد-محور (Event-Driven Serverless Functions)” به صورت ناهمزمان آن‌ها را پردازش کنند. یکی از این رویدادها می‌تواند ارسال یک ایمیل تأیید پس از انجام یک عمل توسط کاربر در سایت باشد. در حالی که توابع سرورلس فرانت‌اند مستقیماً بر زمان پاسخ درک‌شده توسط کاربر تأثیر می‌گذارند و بنابراین دارای یک هدف سطح سرویس (SLO) سخت‌گیرانه برای تأخیر هستند، توابع سرورلس رویداد-محور به صورت ناهمزمان کار می‌کنند و بر زمان پاسخ درک‌شده توسط کاربر تأثیری ندارند، و برای توان عملیاتی (Throughput) و بهره‌وری سخت‌افزاری بهینه‌سازی شده‌اند، نه تأخیر. نسبت سرورهایی که توابع سرورلس فرانت‌اند را اجرا می‌کنند به سرورهایی که توابع سرورلس رویداد-محور را اجرا می‌کنند، تقریباً ۵ به ۱ است.\nپردازش آفلاین اجزای سمت راست شکل ۲ پردازش‌های آفلاین مختلفی را انجام می‌دهند تا به پردازش آنلاین در سمت چپ کمک کنند. جداسازی پردازش آنلاین و آفلاین امکان بهینه‌سازی مستقل بر اساس ویژگی‌های بار کاری هر یک را فراهم می‌کند. هنگام پردازش درخواست‌های کاربر، توابع سرورلس فرانت‌اند و سرویس‌های بک‌اند انواع مختلفی از داده‌ها، مانند معیارهای کلیک روی تبلیغات یا تماشای ویدیو، را در “انبار داده (Data Warehouse)” ثبت می‌کنند. این داده‌ها به عنوان ورودی برای پردازش‌های آفلاین مختلف استفاده می‌شوند. به عنوان مثال، “آموزش یادگیری ماشین (ML Training)” از این داده‌ها برای به‌روزرسانی مدل‌های یادگیری ماشین استفاده می‌کند، در حالی که “پردازش جریان (Stream Processing)” می‌تواند از این داده‌ها برای به‌روزرسانی موضوعات داغ سایت و ذخیره آن‌ها در “پایگاه‌های داده و کش‌ها (Databases and Caches)” استفاده کند، که سپس در طول پردازش درخواست‌های آنلاین کاربر مورد استفاده قرار می‌گیرند. علاوه بر این، “تحلیل دسته‌ای (Batch Analytics)” که توسط Spark و Presto پشتیبانی می‌شود، می‌تواند به طور دوره‌ای عملیاتی مانند به‌روزرسانی توصیه‌های دوستان در پاسخ به فعالیت‌های جدید در سایت را انجام دهد. در نهایت، به‌روزرسانی‌های داده در انبار داده به عنوان منبع اصلی رویداد عمل می‌کند که اجرای توابع سرورلس رویداد-محور را فعال می‌کند.\nشکل۲-معماری کلی اجزای نرم‌افزاری که در یک دیتاسنتر منطقه‌ای اجرا می‌شوند. این دیاگرام به‌شدت ساده‌سازی شده است، زیرا متا به‌صورت داخلی بیش از ۱۰,۰۰۰ سرویس بک‌اند دارد که دارای یک گراف فراخوانی پیچیده هستند.\nافزایش بهره‌وری توسعه‌دهندگان هدف اصلی یک زیرساخت مشترک، افزایش بهره‌وری توسعه‌دهندگان است. در حالی که به طور گسترده‌ای پذیرفته شده است که استقرار مداوم نرم‌افزار (continuous software deployment) و توابع سرورلس (serverless) می‌توانند به افزایش بهره‌وری توسعه‌دهندگان کمک کنند، ما این رویکردها را به حد افراط رسانده‌ایم.\nاستقرار مداوم همسو با فرهنگ سرعت بالا، ما استقرار مداوم کد و پیکربندی را با سرعت و مقیاس بسیار بالا انجام می‌دهیم. این رویکرد به توسعه‌دهندگان امکان می‌دهد تا ویژگی‌های جدید و اصلاحات را به‌سرعت منتشر کنند، بازخورد فوری دریافت کنند و به‌صورت سریع و مداوم بهبودهای لازم را اعمال کنند.\nبرای تغییرات پیکربندی، ابزار مدیریت پیکربندی ما روزانه بیش از ۱۰۰,۰۰۰ تغییر زنده را در محیط تولید اجرا می‌کند که شامل O(10,000) سرویس و میلیون‌ها سرور می‌شود. این تغییرات وظایف مختلفی را تسهیل می‌کنند، از جمله توازن بار، عرضه ویژگی‌ها، تست‌های A/B و محافظت در برابر اضافه‌بار. در متا، تقریباً هر مهندسی که کد می‌نویسد، تغییرات پیکربندی زنده را نیز در محیط تولید اعمال می‌کند. با پیروی از پارادایم “پیکربندی به عنوان کد”، تغییرات دستی پیکربندی قبل از ثبت در مخزن کد، توسط هم‌تیمی‌ها بررسی می‌شوند. پس از ثبت، این تغییرات بلافاصله وارد خط لوله استقرار مداوم می‌شوند. در عرض چند ثانیه، پیکربندی به‌روزرسانی‌شده می‌تواند به میلیون‌ها فرآیند لینوکس مشترک ارسال شود و یک اعلان upcall را فعال کند. فرآیندها می‌توانند بلافاصله رفتار زمان اجرای خود را بدون نیاز به راه‌اندازی مجدد تنظیم کنند. علاوه بر تغییرات دستی، ابزارهای خودکار نیز تغییرات پیکربندی را هدایت می‌کنند، مثلاً برای توازن بار.\nبرای تغییرات کد، ابزار استقرار ما بیش از ۳۰,۰۰۰ خط لوله را برای استقرار ارتقاء نرم‌افزار مدیریت می‌کند. در متا، ۹۷٪ از سرویس‌ها از استقرار کاملاً خودکار نرم‌افزار بدون هیچ گونه مداخله دستی استفاده می‌کنند: ۵۵٪ از استقرار مداوم استفاده می‌کنند و هر تغییر کد را بلافاصله پس از گذراندن تست‌های خودکار به محیط تولید منتشر می‌کنند، در حالی که ۴۲٪ باقی‌مانده به طور خودکار در یک برنامه ثابت، معمولاً روزانه یا هفتگی، مستقر می‌شوند. به عنوان مثال، توابع سرورلس frontend در شکل ۲ را در نظر بگیرید. این توابع بر روی بیش از نیم میلیون سرور اجرا می‌شوند و بیش از ۱۰,۰۰۰ توسعه‌دهنده محصول هر روز کاری کد آن‌ها را تغییر می‌دهند و هزاران commit کد انجام می‌شود. با وجود این محیط بسیار پویا، هر سه ساعت یک بار نسخه جدیدی از تمام توابع سرورلس به محیط تولید منتشر می‌شود.\nحتی نرم‌افزار شبکه ما نیز مانند سرویس‌های معمولی طراحی شده و برای به‌روزرسانی‌های مکرر بهینه‌سازی شده است. به عنوان مثال، شبکه خصوصی WAN ما توپولوژی شبکه را به چندین صفحه موازی تقسیم می‌کند که هر کدام مسئول بخشی از ترافیک هستند و کنترلر خود را دارند. این امر امکان به‌روزرسانی‌های مکرر نرم‌افزار کنترلر را فراهم می‌کند. توسعه‌دهندگان می‌توانند با هدایت ترافیک از یک صفحه و استقرار الگوریتم جدید فقط در آن صفحه، بدون تأثیر بر سایر صفحات، الگوریتم‌های کنترل جدید را آزمایش کنند. به طور مشابه، نرم‌افزار سوئیچ شبکه ما نیز مانند سرویس‌های استاندارد به‌روزرسانی‌های مکرر را انجام می‌دهد. با استفاده از ویژگی “warm boot” در ASIC سوئیچ، صفحه داده به انتقال ترافیک ادامه می‌دهد در حالی که نرم‌افزار سوئیچ در حال به‌روزرسانی است.\nبه‌روزرسانی‌های مکرر کد و پیکربندی، توسعه چابک نرم‌افزار را ممکن می‌سازد اما خطر قطعی سایت را افزایش می‌دهد. برای مقابله با این خطر، ما سرمایه‌گذاری زیادی در تست‌ها، عرضه‌های مرحله‌ای و بررسی‌های سلامت در طول به‌روزرسانی‌ها انجام داده‌ایم. پیش‌تر، یک کمپین شرکت‌محور برای افزایش خودکارسازی استقرار کد راه‌اندازی کردیم که باعث شد پذیرش استقرار کاملاً خودکار کد محافظت‌شده توسط بررسی‌های سلامت از ۱۲٪ به ۹۷٪ افزایش یابد. به طور مشابه، ابتکار دیگری را اجرا کردیم تا اطمینان حاصل کنیم که تمام تغییرات پیکربندی تحت تست‌های خودکار canary قرار می‌گیرند تا ایمنی پیکربندی حفظ شود. به طور کلی، ما این سرمایه‌گذاری‌ها در استقرار مداوم را ارزشمند می‌دانیم، زیرا به طور قابل توجهی بهره‌وری توسعه‌دهندگان را افزایش می‌دهد.\nتوابع سرورلس (Serverless functions): استفاده گسترده از توابع سرورلس (که به عنوان Function-as-a-Service یا FaaS نیز شناخته می‌شوند) یکی دیگر از عوامل کلیدی است که بهره‌وری توسعه‌دهندگان را افزایش می‌دهد. برخلاف سرویس‌های بک‌اند سنتی که می‌توانند پیچیدگی‌های زیادی داشته باشند، FaaS بدون حالت (stateless) است و یک رابط(interface) ساده برای اجرای توابع ارائه می‌دهد. هر فراخوانی FaaS به طور مستقل مدیریت می‌شود و هیچ اثر جانبی بر فراخوانی‌های همزمان دیگر ندارد، مگر از طریق حالت‌هایی که در پایگاه‌های داده خارجی ذخیره شده‌اند. به دلیل ماهیت بدون حالت FaaS، این سیستم به شدت به سیستم‌های کش خارجی متکی است تا در هنگام دسترسی به پایگاه‌های داده، عملکرد خوبی را ارائه دهد.\nتوسعه‌دهندگان کد FaaS را می‌نویسند و بقیه کارها را به زیرساخت می‌سپارند تا از طریق اتوماسیون انجام شود، از جمله استقرار کد و مقیاس‌دهی خودکار در پاسخ به تغییرات بار. این سادگی به بیش از ۱۰,۰۰۰ توسعه‌دهنده محصول متا اجازه می‌دهد تا تنها بر روی منطق محصول تمرکز کنند و نگران مدیریت زیرساخت نباشند. علاوه بر این، این رویکرد از هدررفت منابع سخت‌افزاری ناشی از تأمین بیش از حد منابع توسط توسعه‌دهندگان محصول جلوگیری می‌کند.\nمتا استفاده از FaaS را به حد نهایی رسانده تا بهره‌وری توسعه‌دهندگان را به حداکثر برساند. از بین حدود ۱۰,۰۰۰ مهندس در متا، تعداد مهندسانی که کد FaaS می‌نویسند حدود ۵۰٪ بیشتر از کسانی است که کد برای سرویس‌های معمولی که خودشان مدیریت می‌کنند می‌نویسند. این موفقیت نه تنها به دلیل خلاصی مهندسان محصول از[شرّ] مدیریت زیرساخت، بلکه به دلیل قابلیت استفاده محیط توسعه یکپارچه (IDE) برای FaaS است. این IDE دسترسی آسان به پایگاه‌ داده گراف اجتماعی و سیستم‌های بک‌اند مختلف را از طریق ساختارهای زبان سطح بالا فراهم می‌کند. همچنین بازخورد سریع را از طریق تست‌های یکپارچه‌سازی مداوم ارائه می‌دهد.\nهمان‌طور که در شکل ۲ نشان داده شده است، متا دو پلتفرم FaaS را اداره می‌کند: یکی برای «توابع سرورلس فرانت‌اند» و دیگری برای «توابع سرورلس رویداد-محور». ما به ترتیب به آن‌ها FrontFaaS و XFaaS می‌گوییم. توابع FrontFaaS در PHP نوشته می‌شوند (ما پلتفرم‌های FaaS برای توابع پایتون، ارلنگ و Haskell نیز داریم). برای پشتیبانی از بار بالای تولید شده توسط میلیاردها کاربر، بیش از نیم میلیون سرور را نگهداری می‌کنیم که زمان اجرای PHP را همیشه فعال نگه می‌دارند. هنگامی که یک درخواست کاربر می‌رسد، به یکی از این سرورها هدایت می‌شود تا بلافاصله پردازش شود، بدون اینکه زمان شروع سرد (cold start) را تجربه کند. هنگامی که بار سایت کم است، از مقیاس‌دهی خودکار استفاده می‌کنیم تا برخی از سرورهای FrontFaaS را برای استفاده سایر سرویس‌ها آزاد کنیم.\nXFaaS شباهت‌های زیادی با FrontFaaS دارد، با این تفاوت کلیدی که توابعی را اجرا می‌کند که به کاربر نهایی مربوط نمی‌شوند و نیازی به زمان پاسخ کمتر از یک ثانیه ندارند، اما الگوی بار بسیار ناگهانی (spiky) دارند. برای جلوگیری از تحمیل بیش‌ از حد بار روی زیرساخت‌ها، XFaaS از ترکیبی از بهینه‌سازی‌ها برای گسترش اجرای توابع استفاده می‌کند، از جمله به تعویق انداختن اجرای توابع تحمل‌پذیر، تأخیر به ساعات کم‌بار، متعادل‌سازی بار جهانی فراخوانی‌های توابع در مناطق مختلف و اعمال محدودیت بر اساس سهمیه‌ها.\nتوسعه‌دهندگان محصول در متا از اواخر دهه ۲۰۰۰ از FaaS به عنوان پارادایم اصلی کدنویسی خود استفاده کرده‌اند، حتی قبل از اینکه اصطلاح FaaS رایج شود. در مقایسه با پلتفرم‌های سرورلس در صنعت، یک جنبه منحصر به فرد پلتفرم‌های سرورلس ما این است که به چندین تابع اجازه می‌دهند به طور همزمان در یک فرآیند لینوکس اجرا شوند تا کارایی سخت‌افزاری بالاتری داشته باشند، برخلاف ابرهای عمومی که برای اطمینان از جداسازی قوی‌تر بین مشتریان مختلف، مجبورند یک تابع را در هر ماشین مجازی اجرا کنند.\nکاهش هزینه‌های سخت‌افزاری علاوه بر افزایش بهره‌وری توسعه‌دهندگان، هدف اصلی دیگر یک زیرساخت مشترک، کاهش هزینه‌های سخت‌افزاری است. در این بخش، به چند نمونه از نحوه کمک راه‌حل‌های نرم‌افزاری به کاهش هزینه‌های سخت‌افزاری اشاره می‌کنیم.\nتمامی دیتاسنترهای جهانی به‌عنوان یک رایانه واحد بیشتر ارائه دهندگان زیرساخت‌، مدیریت پیچیدگی‌های مربوط به دیتاسنترهای توزیع‌شده جغرافیایی را بر عهده‌ی کاربران می‌گذارند. کاربران باید به‌صورت دستی تعداد نسخه‌های کپی (replicas) سرویس‌های خود را تعیین کرده و مناطق مناسب برای استقرار را انتخاب کنند، درحالی‌که همچنان باید الزامات سطح سرویس (SLO) را رعایت کنند. این پیچیدگی اغلب منجر به هدررفت سخت‌افزار به دلیل تخصیص بیش از حد منابع، توزیع نامتعادل بار بین مناطق مختلف، و مهاجرت ناکافی سرویس‌ها بین مناطق برای انطباق با تغییرات در میزان تقاضای پردازشی و ظرفیت دیتاسنترها می‌شود.\nدر مقابل، متا در حال حرکت از رویکرد “دیتاسنتر به عنوان یک کامپیوتر” (DaaC) به سمت چشم‌انداز “تمامی دیتاسنترها در (سطح) جهان به عنوان یک کامپیوتر” (Global-DaaC) است. با Global-DaaC، کاربران به سادگی درخواست استقرار جهانی یک سرویس را می‌دهند و زیرساخت تمام جزئیات را مدیریت می‌کند: تعیین تعداد بهینه رپلیکاهای سرویس، قرار دادن این رپلیکاها در دیتاسنترهای منطقه‌ای بر اساس اهداف سطح سرویس و سخت‌افزار موجود، انتخاب نوع سخت‌افزار مناسب‌ترین، بهینه‌سازی مسیریابی ترافیک و تطبیق مداوم جایگاه سرویس در پاسخ به تغییرات بار کاری. در مقایسه با ابرهای عمومی، متا می‌تواند Global-DaaC را راحت‌تر محقق کند زیرا مالک تمام برنامه‌های خود است و می‌تواند آن‌ها را در صورت نیاز بین مناطق جابجا کند؛ در حالی که ابرهای عمومی این انعطاف‌پذیری را با برنامه‌های مشتریان خود ندارند.\nبرای پیاده‌سازی Global-DaaC، ابزارهای ما به طور یکپارچه تخصیص منابع را در تمام سطوح هماهنگ می‌کنند: جهانی، منطقه‌ای و سرورهای انحصاری. ابتدا، ابزار مدیریت ظرفیت جهانی ما با استفاده از ردیابی RPC وابستگی‌های سرویس را شناسایی کرده و مدل‌های مصرف منابع را می‌سازد، سپس از برنامه‌ریزی عدد صحیح مختلط (Mixed-Integer Programming) برای تقسیم نیازهای ظرفیت جهانی یک سرویس به سهمیه‌های منطقه‌ای استفاده می‌کند. در مرحله بعد، ابزار مدیریت ظرفیت منطقه‌ای ما منابع سرور را به این سهمیه‌های منطقه‌ای اختصاص می‌دهد تا خوشه‌های مجازی (Virtual Clusters) تشکیل دهد. برخلاف خوشه‌های فیزیکی، یک خوشه مجازی می‌تواند شامل سرورهایی از مراکز داده مختلف در یک منطقه باشد و اندازه آن می‌تواند به صورت پویا افزایش یا کاهش یابد. در زمان اجرا، ابزار مدیریت کانتینر ما کانتینرها را در این خوشه‌های مجازی تخصیص می‌دهد، که اغلب کانتینرهای یک کار را در چندین مرکز داده در یک منطقه پخش می‌کند تا تحمل خطا (Fault Tolerance) بهبود یابد. در نهایت، در سطح سرور، مکانیزم‌های کرنل ما اطمینان حاصل می‌کنند که حافظه و منابع I/O اختصاص داده شده به کانتینرهای فردی به درستی به اشتراک گذاشته شده و ایزوله شوند.\nسرویس‌های stateful، مانند پایگاه‌های داده از Global-DaaC بهره می‌برند. این سرویس‌ها معمولاً به‌صورت sharded اجرا می‌شوند، به این معنا که هر کانتینر برای افزایش بهره‌وری، چندین بخش داده (shard) را میزبانی می‌کند. Global Service Placer(GSP) ما از الگوریتم‌های بهینه‌سازی با در نظر گرفتن محدودیت‌ها برای تعیین تعداد بهینه‌ی نسخه‌های کپی (replicas) برای هر بخش داده و توزیع آن‌ها در مناطق مختلف استفاده می‌کند. سپس، چارچوب شاردینگ ما در محدوده‌ی این قوانین عمل کرده و نسخه‌های داده را بین کانتینرها تخصیص می‌دهد و آن‌ها را به‌صورت پویا جابه‌جا می‌کند تا با تغییرات بار پردازشی سازگار شود.\nبه طور مشابه، بارکاری یادگیری ماشین (ML) نیز از Global-DaaC بهره می‌برند. برای استنتاج ML، مدل‌ها مشابه shardهای داده مدیریت می‌شوند، و تعداد رپلیکاهای مدل و مکان آن‌ها توسط GSP تعیین می‌شود. برای آموزش ML، نیاز به هم‌مکانی (Collocation) داده‌های آموزشی و GPUها در یک منطقه مرکز داده است. هر تیم یک سهمیه ظرفیت جهانی GPU دریافت می‌کند و کارهای آموزشی را به یک صف کار جهانی ارسال می‌کند. زمان‌بند آموزش ML ما به طور خودکار مناطق را برای تکثیر داده و تخصیص GPU انتخاب می‌کند تا هم‌مکانی داده و GPUها را تضمین کند و در عین حال بهره‌وری GPU را به حداکثر برساند.\nطراحی مشارکتی سخت‌افزار و نرم‌افزار در حالی که طراحی مشارکتی سخت‌افزار و نرم‌افزار در سطح یک سرور واحد رایج است، ما این مفهوم را به مقیاس جهانی ارتقا داده‌ایم تا با استفاده از راه‌حل‌های نرم‌افزاری، محدودیت‌های سخت‌افزاری را با هزینه‌ی کمتر برطرف کنیم.\nتحمل خطای کم‌هزینه ابرهای عمومی تمایل دارند سخت‌افزاری با دسترسی بالاتر ارائه دهند، زیرا برنامه‌های مشتریان آن‌ها ممکن است به اندازه کافی در برابر خطا مقاوم نباشند. در مقابل، از آنجا که تمام برنامه‌های ما تحت کنترل ما هستند، می‌توانیم اطمینان حاصل کنیم که آن‌ها به گونه‌ای پیاده‌سازی شده‌اند که در برابر خطا مقاوم باشند و روی سخت‌افزار ارزان‌تر با تضمین‌های دسترسی پایین‌تر اجرا شوند. به عنوان مثال، یک رک سرور در ابرهای عمومی ممکن است از دو منبع تغذیه و دو سوئیچ بالای رک (ToR) استفاده کند تا دسترسی بالا را تضمین کند و تعمیر و نگهداری سوئیچ‌ها بدون اختلال در بارهای کاری در حال اجرا انجام شود. در مقابل، رک‌های ما نه منبع تغذیه دوگانه دارند و نه سوئیچ‌های ToR دوگانه. در عوض، افزونگی سخت‌افزاری تنها در سطح بسیار بزرگ‌تری مانند تابلوهای اصلی سوئیچ (MSB) اتفاق می‌افتد، که هر کدام حدود ۱۰,۰۰۰ تا ۲۰,۰۰۰ سرور را پوشش می‌دهند. برای هر شش MSB، تنها یک MSB رزرو به عنوان پشتیبان وجود دارد. علاوه بر این، ماشین‌های مجازی (VM) در ابرهای عمومی اغلب از دستگاه‌های بلوک متصل به شبکه استفاده می‌کنند، که امکان مهاجرت زنده VM را فراهم می‌کند. در مقابل، کانتینرهای ما از SSDهای متصل مستقیم و کم‌هزینه برای دیسک‌های ریشه استفاده می‌کنند، که مهاجرت زنده کانتینر را در طول عملیات نگهداری مرکز داده دشوار می‌کند.\nما از راه‌حل‌های نرم‌افزاری برای غلبه بر محدودیت‌های سخت‌افزار کم‌هزینه استفاده می‌کنیم. اولاً، ابزارهای تخصیص منابع ما اطمینان می‌دهند که کانتینرها و shardهای داده یک سرویس به اندازه کافی در دامنه‌های خطای زیر-مرکز داده (MSBها) پخش شده‌اند تا تحمل خطا بهبود یابد. ثانیاً، از طریق یک پروتکل همکاری که به یک سرویس اجازه می‌دهد در مدیریت چرخه عمر کانتینرهای خود مشارکت کند، اطمینان حاصل می‌کنیم که عملیات نگهداری محدودیت‌های سطح برنامه را رعایت می‌کنند، مانند جلوگیری از خاموشی همزمان دو رپلیکا از یک shard داده. در نهایت، Global-DaaC اطمینان می‌دهد که سرویس‌ها به گونه‌ای مستقر می‌شوند که بتوانند از دست دادن همزمان یک منطقه کامل مرکز داده، یک MSB در هر منطقه و درصدی از سرورهای تصادفی در هر منطقه را تحمل کنند. ما به طور معمول تست‌هایی در محیط عملیاتی انجام می‌دهیم تا اطمینان حاصل کنیم که این ویژگی‌ها حفظ می‌شوند و سرویس‌های ما در برابر خطا مقاوم هستند.\nدر حالی که زیرساخت ما به گونه‌ای طراحی شده است که از دست دادن یک منطقه کامل مرکز داده را بدون تأثیر بر کاربران تحمل کند، افزایش تعداد مناطق احتمال تحت تأثیر قرار گرفتن همزمان دو منطقه نزدیک به هم توسط یک فاجعه طبیعی بزرگ، مانند طوفان، را افزایش داده است. به جای تأمین بیش از حد ظرفیت برای تحمل از دست دادن همزمان دو منطقه، ما از یک رویکرد نرم‌افزاری استفاده می‌کنیم که در صورت از دست دادن چندین منطقه، ویژگی‌های کم‌اهمیت‌تر محصول را غیرفعال کرده و کیفیت سرویس را به طور کنترل‌شده کاهش می‌دهد، مانند ارائه ویدیوهای با کیفیت پایین‌تر، تا فشار روی زیرساخت‌ها کاهش یابد.\nحذف هزینه‌های پروکسی‌های مسیریابی برخلاف شبکه‌های سرویس سنتی که عمدتاً از پروکسی‌های sidecar برای مسیریابی درخواست‌های RPC استفاده می‌کنند، شبکه سرویس متا از پروکسی‌ها تنها برای مسیریابی ۱٪ از درخواست‌های RPC در ناوگان ما استفاده می‌کند. ۹۹٪ باقی‌مانده از یک کتابخانه مسیریابی استفاده می‌کنند که به اجرایی‌های سرویس لینک شده است و مسیریابی مستقیم از کلاینت به سرور را انجام می‌دهد، بدون نیاز به پروکسی‌های واسطه. در حالی که این رویکرد غیرمعمول باعث صرفه‌جویی در O(100,000) سرور مورد نیاز برای پروکسی‌ها می‌شود، چالش‌های استقرار را به دلیل کامپایل شدن کتابخانه در حدود O(10,000) سرویس، هر کدام با برنامه استقرار خود، افزایش می‌دهد. ابزارهای استقرار نرم‌افزار و مدیریت پیکربندی ما به مدیریت این چالش‌ها کمک می‌کنند.\nذخیره‌سازی لایه‌ای و SSDهای محلی بر اساس فرکانس دسترسی و تحمل تأخیر، داده‌ها را به سه دسته داغ (Hot)، گرم (Warm) و سرد (Cold) تقسیم می‌کنیم، که هر دسته از یک سیستم ذخیره‌سازی متفاوت برای بهینه‌سازی هزینه-کارایی استفاده می‌کند. پایگاه‌های داده و کش‌های داغ، مانند پایگاه داده گراف اجتماعی، داده‌ها را در حافظه و درایوهای حالت جامد (SSD) ذخیره می‌کنند.\nداده‌های گرم، شامل ویدیوها، تصاویر و داده‌های موجود در انبار داده (مانند لاگ‌های فعالیت کاربر)، در یک سیستم فایل توزیع‌شده ذخیره می‌شوند که از درایوهای دیسک سخت (HDD) برای ذخیره داده استفاده می‌کند. هر سرور ذخیره‌سازی مجهز به یک CPU، ۳۶ HDD و دو SSD برای کش متادیتا است.\nبرای داده‌های سرد که به ندرت به آن‌ها دسترسی می‌شود، مانند یک ویدیوی با وضوح بالا که ده سال پیش ذخیره شده است، آن‌ها را در سرورهای HDD با چگالی بالا آرشیو می‌کنیم، که هر کدام دارای یک CPU و ۲۱۶ HDD هستند و تعادل خوبی بین هزینه کل مالکیت و سرعت بازیابی داده ارائه می‌دهند. این HDDها بیشتر اوقات خاموش هستند، زیرا در حال استفاده فعال نیستند.\nدر میان بارهای کاری که داده‌ها را روی SSD ذخیره می‌کنند، برخی می‌توانند تأخیرهای طولانی‌تر را تحمل کنند و برای استفاده بهتر از SSD، از ذخیره‌سازی اشتراکی مبتنی بر SSD استفاده می‌کنند. با این حال، بارهای کاری با نیازهای سخت‌گیرانه به تأخیر همچنان از SSDهای محلی متصل مستقیم استفاده می‌کنند. در مقایسه با سایر زیرساخت‌های هایپرسکیل، ما بیشتر از SSDهای محلی برای کاهش هزینه‌ها استفاده می‌کنیم، علیرغم پیچیدگی‌های مدیریتی که به همراه دارد. به عنوان مثال، توزیع نابرابر بار می‌تواند منجر به استفاده ناکافی و به‌دردنخور شدن SSDهای محلی شود. علاوه بر این، بازیابی پس از خطا به دلیل گیر کردن داده‌ها در SSDهای سرورهای خراب، پیچیده می‌شود. برای حل این چالش‌ها، از چارچوب sharding مشترک خود برای پیاده‌سازی سرویس‌های stateful با SSDهای محلی استفاده می‌کنیم، که این مشکلات را یک بار حل کرده و راه‌حل را در بسیاری از سرویس‌ها استفاده مجدد می‌کنیم.\nطراحی اختصاصی سخت‌افزار ما مراکز داده و سخت‌افزارهای خود—از جمله سرورها، سوئیچ‌های شبکه، شتاب‌دهنده‌های ویدیو و تراشه‌های هوش مصنوعی—را برای کاهش هزینه‌ها و بهبود بهره‌وری انرژی خودمان طراحی می‌کنیم. در مراکز داده، تامین برق بزرگ‌ترین محدودیت است، زیرا ظرفیت تأمین برق در زمان ساخت دیتاسنتر مشخص می‌شود و افزایش آن در طول عمر ۲۰ تا ۳۰ ساله‌ی دیتاسنتر بسیار دشوار است. در مقابل، شبکه و سرورها را می‌توان به‌روز کرد. برق در دیتاسنترها معمولاً بیش از ظرفیت اسمی تخصیص داده می‌شود (Oversubscribed)، به این معنا که مجموع نیازهای برق سرورها از مقدار واقعی برق قابل تأمین بیشتر در نظر گرفته می‌شود. برای جلوگیری از مصرف بیش از حد برق در زمان افزایش ناگهانی بار پردازشی، یک ابزار خودکار اقدامات محدودسازی مصرف برق را در سطوح مختلف سیستم تأمین برق هماهنگ می‌کند.\nطراحی‌های سخت‌افزاری ما معمولاً از طریق هماهنگی بین سخت‌افزار و نرم‌افزار (مانند بهینه‌سازی استفاده از SRAM در تراشه‌های هوش مصنوعی بر اساس بارهای کاری) و حذف اجزای غیرضروری (مثل حذف سیستم‌های خنک‌کننده با کمپرسور) به صرفه‌جویی در هزینه و انرژی منجر می‌شوند. علاوه بر این، توسعه داخلی سوئیچ‌های شبکه و نرم‌افزارهای مرتبط، این امکان را فراهم می‌کند که نرم‌افزار سوئیچ را به‌عنوان یک سرویس معمولی در نظر گرفته و به‌روزرسانی‌ها را به‌طور مکرر انجام دهیم. بیشتر طراحی‌های سخت‌افزاری ما از طریق پروژه Open Compute به صورت متن‌باز در دسترس هستند.\nطراحی سیستم‌های مقیاس‌پذیر یک موضوع تکرارشونده در زیرساخت‌های هایپرسکیل، طراحی سیستم‌های مقیاس‌پذیر است. سیستم‌های غیرمتمرکز طراحی‌شده برای محیط اینترنت، مانند BGP، BitTorrent و جداول هش توزیع‌شده (DHTs)، اغلب به دلیل مقیاس‌پذیری مورد تحسین قرار می‌گیرند. با این حال، در محیط مرکز داده، که محدودیت منابع کمتری دارد و تحت کنترل یک سازمان واحد است، تجربیات ما نشان می‌دهد که کنترل‌کننده‌های متمرکز نه تنها مقیاس‌پذیری کافی را فراهم می‌کنند، بلکه ساده‌تر هستند و می‌توانند تصمیم‌گیری‌های با کیفیت‌تری انجام دهند.\n**کنار گذاشتن کنترل‌کننده‌های غیرمتمرکز.** در این بخش، چند نمونه از معاوضه بین کنترل‌کننده‌های متمرکز و غیرمتمرکز را بررسی می‌کنیم. برای سوئیچ‌های شبکه در شبکه داخلی مرکز داده ما، اگرچه آن‌ها هنوز از BGP برای سازگاری استفاده می‌کنند، شبکه دارای یک کنترل‌کننده متمرکز است که می‌تواند مسیرهای مسیریابی را در هنگام ازدحام شبکه یا خرابی لینک‌ها بازنویسی کند.\nبه جز BGP، ما تقریباً تمام کنترل‌کننده‌های غیرمتمرکز را به کنترل‌کننده‌های متمرکز منتقل کرده‌ایم. به عنوان مثال، در شبکه WAN خصوصی ما، از RSVP-TE غیرمتمرکز به یک کنترل‌کننده متمرکز منتقل کردیم تا مسیرهای ترافیک ترجیحی را محاسبه کرده و مسیرهای پشتیبان را برای سناریوهای خرابی رایج به‌طور پیش‌گیرانه ایجاد کند. این امر منجر به استفاده کارآمدتر از منابع شبکه و همگرایی سریع‌تر در هنگام خرابی شبکه شده است.\nبرای ذخیره‌سازی کلید-مقدار، DHTها از مسیریابی چند‌هاپ (Multi-Hop) برای تعیین سرور مسئول یک کلید مشخص استفاده می‌کنند، در حالی که Cassandra از هش سازگار (Consistent Hashing) برای این منظور استفاده می‌کند. هر دو بدون نیاز به یک کنترل‌کننده مرکزی عمل می‌کنند. در مقابل، برای دستیابی به تعادل بار بهتر، چارچوب sharding ما از یک کنترل‌کننده مرکزی استفاده می‌کند تا shardهای حاوی کلید را به صورت پویا به سرورها اختصاص دهد.\nبرای توزیع داده‌های حجیم، ما از BitTorrent به Owl مهاجرت کردیم. این تغییر باعث شد که تصمیم‌گیری درباره‌ی اینکه هر همتا (peer) داده را از کجا دریافت کند، به‌صورت متمرکز انجام شود. نتیجه‌ی این کار افزایش قابل‌توجه سرعت دانلود بود.لازم به ذکر است که هم Owl و هم شبکه‌ی WAN خصوصی ما، بخش کنترلی (control plane) را به‌صورت متمرکز مدیریت می‌کنند تا تصمیم‌گیری‌ها بهینه‌تر شوند، اما همچنان از بخش داده‌ای (data plane) غیرمتمرکز برای ارسال و دریافت واقعی داده‌ها استفاده می‌کنند.\nبرای توزیع متادیتاهای کوچک (که در شکل ۴ توضیح بیشتری داده شده است)، در ابتدا از یک درخت توزیع سه‌سطحی پیاده‌سازی‌شده در جاوا استفاده کردیم. گره‌های میانی این درخت، سرورهای پروکسی اختصاصی بودند و گره‌های برگ، مشترک‌های برنامه(application subscribers) بودند که می‌توانستند به صورت پویا به درخت اضافه یا از آن خارج شوند. هنگامی که این پیاده‌سازی دیگر نمی‌توانست مقیاس‌پذیری بیشتری داشته باشد، به یک درخت توزیع همتا به همتا (Peer-to-Peer) منتقل شدیم، جایی که گره‌های میانی نیز مشترک‌های برنامه بودند که داده‌ها را به سایر مشترک‌ها منتقل می‌کردند. با این حال، در میان میلیون‌ها مشترک برنامه، برخی از آن‌ها به دلیل ماهیت غیراختصاصی‌شان، اغلب با مشکلات عملکردی نویزی مواجه می‌شدند. در نتیجه، استفاده از آن‌ها به عنوان گره‌های میانی برای انتقال ترافیک، کمتر قابل اعتماد بود و منجر به اشکال‌زدایی مکرر و زمان‌بر می‌شد. در نهایت، پس از چند سال استفاده در محیط عملیاتی، درخت توزیع همتا به همتا را کنار گذاشتیم و به معماری اصلی که از سرورهای پروکسی اختصاصی استفاده می‌کرد، بازگشتیم. پیاده‌سازی اصلی جاوا را با یک پیاده‌سازی C++ با عملکرد بالاتر جایگزین کردیم، که به خوبی تا ده‌ها میلیون مشترک مقیاس‌پذیر بود.\n**مطالعه موردی: شبکه سرویس مقیاس‌پذیر** در این بخش، از شبکه سرویس متا، ServiceRouter، به عنوان یک مطالعه موردی استفاده می‌کنیم تا طراحی سیستم‌های مقیاس‌پذیر را نشان دهیم و اثبات کنیم که کنترل‌کننده‌های متمرکز در ترکیب با صفحه داده غیرمتمرکز می‌توانند به خوبی در محیط مرکز داده مقیاس‌پذیر باشند. ServiceRouter میلیاردها درخواست RPC در ثانیه را بین میلیون‌ها روتر لایه ۷ (L7، یعنی لایه کاربردی) مسیریابی می‌کند.\nشکل ۳ یک شبکه سرویس رایج در صنعت را نشان می‌دهد، جایی که هر فرآیند سرویس با یک پروکسی sidecar لایه ۷ همراه است که درخواست‌های RPC را برای سرویس مسیریابی می‌کند. به عنوان مثال، هنگامی که سرویس A روی سرور ۱ درخواست‌هایی به سرویس B ارسال می‌کند، پروکسی روی سرور ۱ آن‌ها را بین سرورهای ۲، ۳ و ۴ متعادل می‌کند. در حالی که این راه‌حل به طور گسترده مورد استفاده قرار می‌گیرد، برای زیرساخت‌های هایپرسکیل مقیاس‌پذیر نیست، زیرا کنترل‌کننده مرکزی نمی‌تواند به طور مستقیم جداول مسیریابی میلیون‌ها پروکسی sidecar را پیکربندی کند. کنترل‌کننده مرکزی دو وظیفه دارد: تولید متادیتای مسیریابی جهانی و مدیریت هر روتر لایه ۷. برای مقیاس‌پذیری، ما وظیفه اول را در کنترل‌کننده مرکزی نگه می‌داریم، اما وظیفه دوم را به روترهای لایه ۷ منتقل می‌کنیم، به طوری که هر روتر لایه ۷ بتواند خود را پیکربندی و مدیریت کند.\nمطالعه موردی: شبکه سرویس مقیاس‌پذیر در این بخش، از شبکه سرویس متا به نام ServiceRouter به عنوان یک مطالعه موردی استفاده می‌کنیم تا طراحی سیستم‌های مقیاس‌پذیر را نشان دهیم و ثابت کنیم که ترکیب کنترلرهای متمرکز با یک صفحه داده غیرمتمرکز می‌تواند در محیط‌های دیتاسنتر به خوبی مقیاس‌پذیری داشته باشد. ServiceRouter میلیاردها درخواست RPC (فراخوانی رویه‌ی راه دور) را در هر ثانیه بین میلیون‌ها روتر لایه ۷ (L7، یعنی لایه کاربردی) مسیریابی می‌کند.\nشکل ۳ یک شبکه سرویس رایج در صنعت را نشان می‌دهد که در آن هر فرآیند سرویس با یک پروکسی جانبی (sidecar) L7 همراه است که درخواست‌های RPC را برای سرویس مسیریابی می‌کند. به عنوان مثال، وقتی سرویس A روی سرور ۱ درخواست‌هایی به سرویس B ارسال می‌کند، پروکسی روی سرور ۱ این درخواست‌ها را بین سرورهای ۲، ۳ و ۴ متعادل می‌کند. اگرچه این راه‌حل به طور گسترده استفاده می‌شود، اما برای زیرساخت‌های بسیار بزرگ (hyperscale) مقیاس‌پذیر نیست، زیرا کنترلر مرکزی نمی‌تواند به طور مستقیم جدول‌های مسیریابی میلیون‌ها پروکسی جانبی را پیکربندی کند. کنترلر مرکزی دو وظیفه دارد: تولید ابرداده‌های مسیریابی جهانی و مدیریت هر روتر L7. برای دستیابی به مقیاس‌پذیری، ما وظیفه اول را در کنترلر مرکزی نگه می‌داریم، اما وظیفه دوم را به روترهای L7 منتقل می‌کنیم و هر روتر L7 را خودپیکربند و خودمدیریت می‌کنیم.\nشکل۳-شبکه‌ی سرویس (Service Mesh) مبتنی بر پروکسی سایدکار (Sidecar Proxy).\nشکل ۴ معماری مقیاس‌پذیر ServiceRouter را نشان می‌دهد. در بالای شکل، کنترلرهای مختلف به طور مستقل وظایف متفاوتی مانند ثبت سرویس‌ها، به‌روزرسانی تاخیرهای شبکه اندازه‌گیری شده و محاسبه جدول مسیریابی بین‌منطقه‌ای برای هر سرویس را انجام می‌دهند. هر کنترلر به طور مستقل پایگاه اطلاعات مسیریابی مرکزی (RIB) را به‌روزرسانی می‌کند و نیازی به پیکربندی یا مدیریت تک‌تک روترهای L7 ندارد. RIB یک پایگاه داده مبتنی بر Paxos است و می‌تواند از طریق تقسیم‌بندی (sharding) مقیاس‌پذیری داشته باشد. با کمک RIB، کنترلرها بدون حالت (stateless) می‌شوند و به راحتی از طریق تقسیم‌بندی مقیاس‌پذیری پیدا می‌کنند. به عنوان مثال، چندین نمونه کنترلر می‌توانند به طور همزمان جدول‌های مسیریابی بین‌منطقه‌ای را برای سرویس‌های مختلف محاسبه کنند.\nشکل۴- معماری سطح بالا‌ی ServiceRouter\nدر وسط شکل ۴، لایه توزیع از هزاران نسخه تکثیرشده RIB استفاده می‌کند تا ترافیک خواندن از میلیون‌ها روتر L7 را مدیریت کند. در پایین شکل، هر روتر L7 با هدایت RIB، بدون نیاز به دخالت مستقیم صفحه کنترل، خود را پیکربندی می‌کند. روترهای L7 ناهمگون (heterogeneous) پشتیبانی می‌شوند که می‌توانند شامل متعادل‌کننده‌های بار (load balancers)، سرویس‌هایی با کتابخانه‌های مسیریابی توکار یا پروکسی‌های جانبی باشند.\nبا توجه به ServiceRouter، می‌توانیم با استفاده از تکنیک‌هایی مانند کنترل‌کننده‌های بدون حالت (stateless)، تکه‌تکه کردن (sharding) کنترل‌کننده‌ها و حذف عملکردهای غیرضروری از کنترل‌کننده‌های مرکزی – مثل مدیریت جداگانه‌ی مسیریاب‌های لایه ۷ – به مقیاس‌پذیری خوبی با کنترل‌کننده‌های متمرکز دست پیدا کنیم.\nجهت‌گیری‌های آینده علیرغم پیچیدگی زیرساخت‌های بسیار بزرگ (hyperscale) متا، در اینجا یک مرور کلی و مختصر ارائه کردیم و بر بینش‌های کلیدی حاصل از توسعه آن تأکید نمودیم. در پایان، دیدگاه‌های خود را درباره روندهای احتمالی آینده برای زیرساخت‌های بسیار بزرگ به اشتراک می‌گذاریم.\nهوش مصنوعی (AI): بارکاری هوش مصنوعی به بزرگ‌ترین دسته‌ی بارکاری در مراکز داده تبدیل شده‌اند. پیش‌بینی می‌کنیم که تا پایان این دهه، بیش از نیمی از انرژی دیتاسنترها به بارکاری‌های هوش مصنوعی اختصاص یابد. به دلیل ویژگی‌های متمایز هوش مصنوعی، مانند نیاز به منابع بیشتر و شبکه‌های با پهنای باند بالاتر، انتظار داریم که هوش مصنوعی هر جنبه‌ای از زیرساخت را به طور عمیقی متحول کند. در دو دهه گذشته، زیرساخت‌های بسیار بزرگ عمدتاً با استفاده از رویکرد مقیاس‌افقی (scaling-out) و بهره‌گیری از تعداد زیادی سرور کم‌هزینه موفق شده‌اند. با این حال، خوشه‌های آینده هوش مصنوعی به احتمال زیاد از رویکرد مقیاس‌عمودی (scale-up) استفاده خواهند کرد که در ابررایانه‌های گذشته دیده می‌شد، مانند استفاده از دسترسی مستقیم به حافظه از راه دور (RDMA) روی اترنت برای ارائه شبکه‌ای با پهنای باند بالا و تاخیر کم که برای آموزش مدل‌های یادگیری ماشین در مقیاس بزرگ ضروری است. رویکرد متا به هوش مصنوعی با طراحی همزمان تمام اجزای سیستم، از PyTorch تا مدل‌های یادگیری ماشین، تراشه‌های هوش مصنوعی، شبکه‌ها، دیتاسنترها، سرورها، ذخیره‌سازی، برق و خنک‌کننده متمایز می‌شود.\nسخت‌افزارهای خاص حوزه (Domain-specific hardware): برخلاف روند کاهش تنوع سخت‌افزاری در دهه ۲۰۰۰، پیش‌بینی می‌کنیم که شاهد گسترش سخت‌افزارهای سفارشی و تخصصی برای اهداف مختلف باشیم، مانند آموزش و استنتاج هوش مصنوعی، مجازی‌سازی، رمزگذاری ویدیو، رمزنگاری، فشرده‌سازی، حافظه‌های لایه‌بندی شده و همچنین پردازش درون شبکه و درون ذخیره‌سازی. دلیل این امر آن است مقیاس‌پذیری اقتصادی به شرکت‌های ابرمقیاس این امکان را می‌دهد که سخت‌افزارهای تخصصی را در ابعاد وسیع طراحی و مستقر کنند و هزینه‌ها را کاهش دهند. در نتیجه، این امر فرصت‌های جدیدی را برای پشته نرم‌افزاری فراهم می‌کند تا از یک ناوگان متنوع و پیشرفته به شکلی بهینه بهره‌برداری کند.\nدیتاسنترهای لبه (Edge datacenters): انتظار داریم که استفاده از برنامه‌های متاورس و اینترنت اشیا (IoT) به طور قابل توجهی افزایش یابند. به عنوان مثال، گیمینگ ابری (cloud gaming) رندرینگ گرافیکی را از دستگاه‌های کاربر به سرورهای GPU در دیتاسنترهای لبه منتقل می‌کند و نیازمند تاخیر شبکه کمتر از ۲۵ میلی‌ثانیه است. تقاضا برای پاسخگویی بلادرنگ احتمالاً باعث رشد قابل توجهی در تعداد و اندازه دیتاسنترهای لبه خواهد شد. در نتیجه، صفحه کنترل زیرساخت باید به گونه‌ای تطبیق یابد که بتواند ناوگانی پراکنده‌تر را مدیریت کند، ترجیحاً با بهبود Global-DaaC تا پیچیدگی زیرساخت پراکنده را از دید توسعه‌دهندگان برنامه‌ها پنهان کند.\nبهره‌وری توسعه‌دهندگان: در دو دهه گذشته، ابزارهای اتوماسیون به طور قابل توجهی بهره‌وری مدیران سیستم را افزایش داده‌اند و منجر به نسبت سرور به مدیر بسیار بالاتری شده‌اند. در مقابل، توسعه نرم‌افزارهای عمومی همچنان پرزحمت است و رشد بهره‌وری در این حوزه کندتر بوده است. در این دهه، انتظار داریم که این روند تغییر کند و بهره‌وری توسعه‌دهندگان به دو دلیل به سرعت افزایش یابد: تولید و اشکال‌زدایی کد با کمک هوش مصنوعی، و پارادایم‌های برنامه‌نویسی سرورلس (serverless) کاملاً یکپارچه در زمینه‌های تخصصی. FrontFaaS متا نمونه‌ای از مورد دوم است، و پیش‌بینی می‌کنیم که پارادایم‌های برنامه‌نویسی بسیار مولد برای زمینه‌های تخصصی بیشتری ظهور کنند.\nما پیش‌بینی می‌کنیم که نوآوری سریع در زیرساخت‌های هایپرسکیل که در دو دهه‌ی گذشته شاهد آن بوده‌ایم، در دهه‌ی آینده نیز ادامه یابد، به‌ویژه با پیشرفت‌های هوش مصنوعی. ما ابرشرکت‌ها را تشویق می‌کنیم که دیدگاه‌های خود را به اشتراک بگذارند تا جامعه بتواند به‌صورت جمعی روند پیشرفت را تسریع کند.\n",
  "wordCount" : "7766",
  "inLanguage": "fa",
  "datePublished": "2025-02-16T20:53:59+03:30",
  "dateModified": "2025-02-16T20:53:59+03:30",
  "author":{
    "@type": "Person",
    "name": "mhhio"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mhhio.github.io/posts/metas-hyperscale-infrastructure-overview-and-insights/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "mhhio's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mhhio.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mhhio.github.io/" accesskey="h" title="mhhio&#39;s Blog (Alt + H)">mhhio&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mhhio.github.io/archives" title="آرشیو">
                    <span>آرشیو</span>
                </a>
            </li>
            <li>
                <a href="https://mhhio.github.io/tags/" title="تگ ها">
                    <span>تگ ها</span>
                </a>
            </li>
            <li>
                <a href="https://mhhio.github.io/search/" title="جست و جو">
                    <span>جست و جو</span>
                </a>
            </li>
            <li>
                <a href="https://mhhio.github.io/about" title="درباره‌ی من">
                    <span>درباره‌ی من</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://mhhio.github.io/">خانه</a>&nbsp;»&nbsp;<a href="https://mhhio.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      بررسی زیرساخت ابرمقیاس متا
    </h1>
    <div class="post-meta"><span class="meta-item">
    2025-02-16
</span><span class="meta-item">&nbsp;·&nbsp;37 دقیقه</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">فهرست مطالب</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%d9%81%d8%b1%d9%87%d9%86%da%af-%d9%85%d9%87%d9%86%d8%af%d8%b3%db%8c" aria-label="فرهنگ مهندسی">فرهنگ مهندسی</a><ul>
                        
                <li>
                    <a href="#%d8%b3%d8%b1%d8%b9%d8%aa-%d8%a8%d8%a7%d9%84%d8%a7" aria-label="سرعت بالا">سرعت بالا</a></li>
                <li>
                    <a href="#%d8%a8%d8%a7%d8%b2-%d8%a8%d9%88%d8%af%d9%86-%d9%81%d9%86%d8%a7%d9%88%d8%b1%db%8c" aria-label="باز بودن فناوری">باز بودن فناوری</a></li>
                <li>
                    <a href="#%d8%aa%d8%ad%d9%82%db%8c%d9%82-%d8%af%d8%b1-%d9%85%d8%ad%db%8c%d8%b7-%d8%b9%d9%85%d9%84%db%8c%d8%a7%d8%aa%db%8c" aria-label="تحقیق در محیط عملیاتی">تحقیق در محیط عملیاتی</a></li>
                <li>
                    <a href="#%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa-%d9%85%d8%b4%d8%aa%d8%b1%da%a9" aria-label="زیرساخت مشترک">زیرساخت مشترک</a></li>
                <li>
                    <a href="#%d9%85%d8%b7%d8%a7%d9%84%d8%b9%d9%87-%d9%85%d9%88%d8%b1%d8%af%db%8c-%d9%81%d8%b1%d9%87%d9%86%da%af-%d8%a8%d8%b1%d9%86%d8%a7%d9%85%d9%87-threads" aria-label="مطالعه موردی فرهنگ: برنامه Threads">مطالعه موردی فرهنگ: برنامه Threads</a></li></ul>
                </li>
                <li>
                    <a href="#%d8%ac%d8%b1%db%8c%d8%a7%d9%86-%d8%af%d8%b1%d8%ae%d9%88%d8%a7%d8%b3%d8%aa-%da%a9%d8%a7%d8%b1%d8%a8%d8%b1-%d8%a7%d8%b2-%d8%a7%d8%a8%d8%aa%d8%af%d8%a7-%d8%aa%d8%a7-%d8%a7%d9%86%d8%aa%d9%87%d8%a7" aria-label="جریان درخواست کاربر از ابتدا تا انتها">جریان درخواست کاربر از ابتدا تا انتها</a><ul>
                        
                <li>
                    <a href="#%d9%85%d8%b3%db%8c%d8%b1%db%8c%d8%a7%d8%a8%db%8c-%d8%af%d8%b1%d8%ae%d9%88%d8%a7%d8%b3%d8%aa" aria-label="مسیریابی درخواست">مسیریابی درخواست</a><ul>
                        
                <li>
                    <a href="#%d9%86%da%af%d8%a7%d8%b4%d8%aa-%d8%af%db%8c%d9%86%d8%a7%d9%85%db%8c%da%a9-dns" aria-label="نگاشت دینامیک DNS">نگاشت دینامیک DNS</a></li>
                <li>
                    <a href="#%da%a9%d8%b4-%d9%85%d8%ad%d8%aa%d9%88%d8%a7%db%8c-%d8%a7%d8%b3%d8%aa%d8%a7%d8%aa%db%8c%da%a9" aria-label="کش محتوای استاتیک">کش محتوای استاتیک</a></li>
                <li>
                    <a href="#%d9%85%d8%b3%db%8c%d8%b1%db%8c%d8%a7%d8%a8%db%8c-%d8%af%d8%b1%d8%ae%d9%88%d8%a7%d8%b3%d8%aa-%d9%85%d8%ad%d8%aa%d9%88%d8%a7%d9%87%d8%a7%db%8c-%d9%be%d9%88%db%8c%d8%a7" aria-label="مسیریابی درخواست محتواهای پویا">مسیریابی درخواست محتواهای پویا</a></li></ul>
                </li>
                <li>
                    <a href="#%d8%aa%d9%88%d9%be%d9%88%d9%84%d9%88%da%98%db%8c-%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa" aria-label="توپولوژی زیرساخت">توپولوژی زیرساخت</a><ul>
                        
                <li>
                    <a href="#%d8%b4%d8%a8%da%a9%d9%87-%d9%84%d8%a8%d9%87-edge-network" aria-label="شبکه لبه (Edge Network)">شبکه لبه (Edge Network)</a></li>
                <li>
                    <a href="#%d8%b4%d8%a8%da%a9%d9%87-%d9%85%d8%b1%da%a9%d8%b2-%d8%af%d8%a7%d8%af%d9%87-datacenter-network" aria-label="شبکه مرکز داده (Datacenter Network)">شبکه مرکز داده (Datacenter Network)</a></li>
                <li>
                    <a href="#%d8%b4%d8%a8%da%a9%d9%87-%d9%85%d9%86%d8%b7%d9%82%d9%87%d8%a7%db%8c-regional-network" aria-label="شبکه منطقه‌ای (Regional Network)">شبکه منطقه‌ای (Regional Network)</a></li></ul>
                </li>
                <li>
                    <a href="#%d9%be%d8%b1%d8%af%d8%a7%d8%b2%d8%b4-%d8%af%d8%b1%d8%ae%d9%88%d8%a7%d8%b3%d8%aa" aria-label="پردازش درخواست">پردازش درخواست</a><ul>
                        
                <li>
                    <a href="#%d9%be%d8%b1%d8%af%d8%a7%d8%b2%d8%b4-%d8%a2%d9%86%d9%84%d8%a7%db%8c%d9%86" aria-label="پردازش آنلاین">پردازش آنلاین</a></li>
                <li>
                    <a href="#%d9%be%d8%b1%d8%af%d8%a7%d8%b2%d8%b4-%d8%a2%d9%81%d9%84%d8%a7%db%8c%d9%86" aria-label="پردازش آفلاین">پردازش آفلاین</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%d8%a7%d9%81%d8%b2%d8%a7%db%8c%d8%b4-%d8%a8%d9%87%d8%b1%d9%87%d9%88%d8%b1%db%8c-%d8%aa%d9%88%d8%b3%d8%b9%d9%87%d8%af%d9%87%d9%86%d8%af%da%af%d8%a7%d9%86" aria-label="افزایش بهره‌وری توسعه‌دهندگان">افزایش بهره‌وری توسعه‌دهندگان</a><ul>
                        
                <li>
                    <a href="#%d8%a7%d8%b3%d8%aa%d9%82%d8%b1%d8%a7%d8%b1-%d9%85%d8%af%d8%a7%d9%88%d9%85" aria-label="استقرار مداوم">استقرار مداوم</a></li>
                <li>
                    <a href="#%d8%aa%d9%88%d8%a7%d8%a8%d8%b9-%d8%b3%d8%b1%d9%88%d8%b1%d9%84%d8%b3-serverless-functions" aria-label="توابع سرورلس (Serverless functions):">توابع سرورلس (Serverless functions):</a></li></ul>
                </li>
                <li>
                    <a href="#%da%a9%d8%a7%d9%87%d8%b4-%d9%87%d8%b2%db%8c%d9%86%d9%87%d9%87%d8%a7%db%8c-%d8%b3%d8%ae%d8%aa%d8%a7%d9%81%d8%b2%d8%a7%d8%b1%db%8c" aria-label="کاهش هزینه‌های سخت‌افزاری">کاهش هزینه‌های سخت‌افزاری</a><ul>
                        
                <li>
                    <a href="#%d8%b7%d8%b1%d8%a7%d8%ad%db%8c-%d9%85%d8%b4%d8%a7%d8%b1%da%a9%d8%aa%db%8c-%d8%b3%d8%ae%d8%aa%d8%a7%d9%81%d8%b2%d8%a7%d8%b1-%d9%88-%d9%86%d8%b1%d9%85%d8%a7%d9%81%d8%b2%d8%a7%d8%b1" aria-label="طراحی مشارکتی سخت‌افزار و نرم‌افزار">طراحی مشارکتی سخت‌افزار و نرم‌افزار</a><ul>
                        
                <li>
                    <a href="#%d8%aa%d8%ad%d9%85%d9%84-%d8%ae%d8%b7%d8%a7%db%8c-%da%a9%d9%85%d9%87%d8%b2%db%8c%d9%86%d9%87" aria-label="تحمل خطای کم‌هزینه">تحمل خطای کم‌هزینه</a></li>
                <li>
                    <a href="#%d8%ad%d8%b0%d9%81-%d9%87%d8%b2%db%8c%d9%86%d9%87%d9%87%d8%a7%db%8c-%d9%be%d8%b1%d9%88%da%a9%d8%b3%db%8c%d9%87%d8%a7%db%8c-%d9%85%d8%b3%db%8c%d8%b1%db%8c%d8%a7%d8%a8%db%8c" aria-label="حذف هزینه‌های پروکسی‌های مسیریابی">حذف هزینه‌های پروکسی‌های مسیریابی</a></li>
                <li>
                    <a href="#%d8%b0%d8%ae%db%8c%d8%b1%d9%87%d8%b3%d8%a7%d8%b2%db%8c-%d9%84%d8%a7%db%8c%d9%87%d8%a7%db%8c-%d9%88-ssd%d9%87%d8%a7%db%8c-%d9%85%d8%ad%d9%84%db%8c" aria-label="ذخیره‌سازی لایه‌ای و SSDهای محلی">ذخیره‌سازی لایه‌ای و SSDهای محلی</a></li></ul>
                </li>
                <li>
                    <a href="#%d8%b7%d8%b1%d8%a7%d8%ad%db%8c-%d8%a7%d8%ae%d8%aa%d8%b5%d8%a7%d8%b5%db%8c-%d8%b3%d8%ae%d8%aa%d8%a7%d9%81%d8%b2%d8%a7%d8%b1" aria-label="طراحی اختصاصی سخت‌افزار">طراحی اختصاصی سخت‌افزار</a></li></ul>
                </li>
                <li>
                    <a href="#%d8%b7%d8%b1%d8%a7%d8%ad%db%8c-%d8%b3%db%8c%d8%b3%d8%aa%d9%85%d9%87%d8%a7%db%8c-%d9%85%d9%82%db%8c%d8%a7%d8%b3%d9%be%d8%b0%db%8c%d8%b1" aria-label="طراحی سیستم‌های مقیاس‌پذیر">طراحی سیستم‌های مقیاس‌پذیر</a><ul>
                        
                <li>
                    <a href="#%d9%85%d8%b7%d8%a7%d9%84%d8%b9%d9%87-%d9%85%d9%88%d8%b1%d8%af%db%8c-%d8%b4%d8%a8%da%a9%d9%87-%d8%b3%d8%b1%d9%88%db%8c%d8%b3-%d9%85%d9%82%db%8c%d8%a7%d8%b3%d9%be%d8%b0%db%8c%d8%b1" aria-label="مطالعه موردی: شبکه سرویس مقیاس‌پذیر">مطالعه موردی: شبکه سرویس مقیاس‌پذیر</a></li></ul>
                </li>
                <li>
                    <a href="#%d8%ac%d9%87%d8%aa%da%af%db%8c%d8%b1%db%8c%d9%87%d8%a7%db%8c-%d8%a2%db%8c%d9%86%d8%af%d9%87" aria-label="جهت‌گیری‌های آینده">جهت‌گیری‌های آینده</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>شرکت های ابرمقیاس (Hyperscalers) مانند علی‌بابا، آمازون، بایت‌دنس، گوگل، متا، مایکروسافت و تنسنت، زیرساخت‌های در مقیاس کره‌زمین توسعه داده‌اند تا خدمات ابری، وب یا موبایل را به کاربران جهانی خود ارائه دهند. اگرچه ممکن است بیشتر متخصصان به طور مستقیم چنین زیرساخت‌های هایپرسکیل را نسازند، معتقدیم که یادگیری درباره‌ی آن‌ها مفید است. از نظر تاریخی، بسیاری از فناوری‌های پرکاربرد از محیط‌های پیشرفته سرچشمه گرفته‌اند، از جمله مین‌فریم‌ها (Mainframes) در دهه‌ی 1960 و زیرساخت‌های هایپرسکیل در دو دهه‌ی اخیر. به عنوان مثال، حافظه‌ی مجازی (Virtual Memory) ابتدا در مین‌فریم‌ها به وجود آمد و اکنون حتی در ساعت‌های هوشمند نیز رایج است. به طور مشابه، Kubernetes و PyTorch به ترتیب در گوگل و فیسبوک ایجاد شدند، اما توسط سازمان‌هایی با اندازه‌های مختلف مورد استفاده قرار گرفته‌اند. علاوه بر این فناوری‌های خاص، اصول و درس‌های آموخته شده از زیرساخت‌های هایپرسکیل ممکن است به متخصصان کمک کند تا به طور کلی سیستم‌های بهتری بسازند.</p>
<p>این مقاله مروری کلی بر زیرساخت‌های هایپرسکیل متا ارائه می‌دهد و بر بینش‌های کلیدی حاصل از توسعه‌ی آن، به ویژه در نرم‌افزارهای سیستمی، تمرکز دارد. در موارد مرتبط، تفاوت‌های آن با ابرهای عمومی (Public Clouds) را برجسته می‌کنیم، زیرا محدودیت‌های مختلف منجر به بهینه‌سازی‌های متمایزی شده‌اند. اگرچه بسیاری از دانش ارائه‌شده در اینجا قبلاً در صنعت و جامعه‌ی تحقیقاتی به اشتراک گذاشته شده و به کار گرفته شده است، اما سهم اصلی این مقاله ارائه‌ی یک دیدگاه جامع است که به خوانندگان کمک می‌کند تا یک مدل ذهنی کامل از زیرساخت‌های هایپرسکیل را از ابتدا تا انتها درک کنند.</p>
<h1 id="فرهنگ-مهندسی">فرهنگ مهندسی<a hidden class="anchor" aria-hidden="true" href="#فرهنگ-مهندسی">#</a></h1>
<p>قبل از پرداختن به جزئیات زیرساخت‌های متا، ابتدا به چند جنبه از فرهنگ مهندسی این شرکت اشاره می‌کنیم، زیرا فرهنگ یک سازمان تأثیر زیادی بر فناوری آن دارد.</p>
<h2 id="سرعت-بالا">سرعت بالا<a hidden class="anchor" aria-hidden="true" href="#سرعت-بالا">#</a></h2>
<p>از زمان تأسیس، فیسبوک فرهنگ &ldquo;سرعت بالا&rdquo; را در خود نهادینه کرده و حفظ کرده است، فرهنگی که بر چابکی و تکرار سریع تأکید دارد. این فلسفه در تعهد قوی شرکت به استقرار مداوم نرم‌افزار (Continuous Software Deployment) مشهود است، جایی که آخرین تغییرات کد در اسرع وقت به محیط عملیاتی (Production) منتقل می‌شود. علاوه بر این، مهندسان محصول عمدتاً کدهای خود را در قالب توابع بدون حالت (Stateless) و سرورلس (Serverless) به زبان‌هایی مانند PHP، Python و Erlang می‌نویسند، زیرا این رویکرد سادگی، بهره‌وری و سرعت توسعه و به‌روزرسانی را افزایش می‌دهد. تیم‌ها این امکان را دارند که به‌سرعت اولویت‌های اجرایی خود را تغییر دهند، بدون آنکه نیاز به یک فرایند برنامه‌ریزی طولانی داشته باشند، و می‌توانند مسائل مبهم را در حین اجرای تدریجی حل کنند. این رویکرد به آن‌ها اجازه می‌دهد تا به‌سرعت با شرایط متغیر بازار سازگار شده و محصولات جدیدی را عرضه کنند.</p>
<h2 id="باز-بودن-فناوری">باز بودن فناوری<a hidden class="anchor" aria-hidden="true" href="#باز-بودن-فناوری">#</a></h2>
<p>متا از باز بودن فناوری، هم به صورت داخلی و هم خارجی، حمایت می‌کند. در داخل شرکت، ما از رویکرد مونورپو (Monorepo) استفاده می‌کنیم، جایی که کد تمام پروژه‌ها در یک مخزن واحد ذخیره می‌شود تا یافتن و استفاده مجدد از کد و همچنین مشارکت بین تیم‌ها تسهیل شود. در حالی که سازمان‌های دیگر نیز از مونورپو استفاده می‌کنند، میزان باز بودن آن‌ها در استفاده از این رویکرد متفاوت است. برخی از سازمان‌ها برای هر پروژه مالکانی تعیین می‌کنند و تنها این مالکان مجاز به پذیرش تغییرات کد هستند، اگرچه دیگران می‌توانند تغییرات را پیشنهاد دهند. در مقابل، با چند استثنا، اکثر پروژه‌ها در متا چنین قوانین سختگیرانه‌ای در مورد مالکیت اعمال نمی‌کنند. این باز بودن، مشارکت بین تیم‌ها و استفاده مجدد از کد را تشویق می‌کند و از نوشتن مجدد کدهای مشابه جلوگیری می‌کند.</p>
<p>در متا، مهندسان تغییرات کد را مستقیماً به خط اصلی (Mainline) مونورپو کامیت می‌کنند، و استقرار نرم‌افزارها از خط اصلی، یعنی از آخرین نسخه کد، کامپایل می‌شود، برخلاف برخی شاخه‌های پایدار. به عنوان مثال، هنگامی که یک کتابخانه پراستفاده، مانند کتابخانه RPC، به‌روزرسانی می‌شود، نسخه بعدی هر برنامه‌ای که به این کتابخانه وابسته است، به طور خودکار با آخرین نسخه آن کامپایل می‌شود.</p>
<p>در خارج از شرکت، تعهد متا به باز بودن فناوری از طریق طراحی‌های سخت‌افزاری متن‌باز در پروژه Open Compute و پروژه‌های نرم‌افزاری مانند PyTorch، Llama، Presto، RocksDB و Cassandra نشان داده می‌شود. همچنین، بخش زیادی از فناوری زیرساخت متا از طریق مقالات تحقیقاتی به اشتراک گذاشته شده است، که بسیاری از آن‌ها در مراجع این مقاله ذکر شده‌اند.</p>
<h2 id="تحقیق-در-محیط-عملیاتی">تحقیق در محیط عملیاتی<a hidden class="anchor" aria-hidden="true" href="#تحقیق-در-محیط-عملیاتی">#</a></h2>
<p>زیرساخت‌های هایپرسکیل متا نیازمند نوآوری مداوم است، اما برخلاف بیشتر هایپرسکیلرها، این شرکت یک آزمایشگاه تحقیقاتی سیستم‌های اختصاصی ندارد. در عوض، تمام مقالات تحقیقاتی سیستم‌های آن توسط تیم‌هایی نوشته می‌شود که سیستم‌های عملیاتی را توسعه می‌دهند. این تیم‌ها در حالی که مسائل چالش‌برانگیز عملیاتی را در مقیاس بزرگ حل می‌کنند، مرزهای فناوری را پیش می‌برند و سپس این تجربیات را در قالب مقالات تحقیقاتی منتشر می‌کنند. این رویکرد تضمین می‌کند که مشکلات مطرح شده واقعی هستند و راه‌حل‌ها در مقیاس بزرگ کار می‌کنند، که با معیارهای کلیدی موفقیت در تحقیقات سیستم‌ها هم‌خوانی دارد.</p>
<h2 id="زیرساخت-مشترک">زیرساخت مشترک<a hidden class="anchor" aria-hidden="true" href="#زیرساخت-مشترک">#</a></h2>
<p>در حالی که برخی سازمان‌ها به تیم‌های فردی اجازه می‌دهند تا در مورد پشته فناوری خود تصمیم‌گیری محلی داشته باشند، متا استانداردسازی و بهینه‌سازی جهانی را در اولویت قرار می‌دهد. در بخش سخت‌افزار، سرورهایی که از محصولات مختلف پشتیبانی می‌کنند، همگی از یک استخر سرور مشترک تخصیص داده می‌شوند. علاوه بر این، برای بارهای کاری غیرهوش مصنوعی (Non-AI Compute Workloads)، ما تنها یک نوع سرور ارائه می‌دهیم که مجهز به یک CPU و مقدار مشخصی از DRAM (قبلاً 64 گیگابایت، اکنون 256 گیگابایت) است. برخلاف ابرهای عمومی که باید انواع مختلف سرورها را برای تطبیق با برنامه‌های متنوع مشتریان ارائه دهند، متا می‌تواند برنامه‌های خود را برای تطبیق با سخت‌افزار بهینه‌سازی کند، و در نتیجه از افزایش انواع سرورها جلوگیری کند.</p>
<p>استانداردسازی در بخش نرم‌افزار نیز حاکم است. به عنوان مثال، محصولات مختلف متا قبلاً از Cassandra، HBase و ZippyDB برای ذخیره‌سازی کلید-مقدار (Key-Value Store) استفاده می‌کردند، اما اکنون همگی به ZippyDB روی آورده‌اند. علاوه بر این، هر قابلیت مشترک—مانند استقرار نرم‌افزار، مدیریت پیکربندی، شبکه سرویس (Service Mesh)، تست عملکرد قبل از تولید، نظارت بر عملکرد در محیط عملیاتی، و تست بار در محیط عملیاتی—توسط یک ابزار جهانی که به طور گسترده پذیرفته شده است، پشتیبانی می‌شود.</p>
<p>علاوه بر استانداردسازی، یک اصل کلیدی در دستیابی به زیرساخت مشترک، ترجیح ما برای استفاده از اجزای قابل استفاده مجدد به جای راه‌حل‌های یکپارچه (Monolithic) است. یک مثال خوب از این موضوع، زنجیره استفاده مجدد از اجزا در سیستم فایل توزیع‌شده ما به نام Tectonic است. Tectonic با استفاده از یک ذخیره‌سازی کلید-مقدار توزیع‌شده به نام ZippyDB، مقیاس‌پذیری خود را افزایش می‌دهد. ZippyDB نیز به نوبه خود از یک چارچوب مشترک برای مدیریت قطعات داده (Sharding) به نام Shard Manager استفاده می‌کند. Shard Manager نیز برای کشف قطعات و مسیریابی درخواست‌ها به ServiceRouter متا وابسته است. در نهایت، ServiceRouter داده‌های کشف سرویس و پیکربندی کل زیرساخت را در یک فضای ذخیره‌سازی داده بسیار قابل اعتماد و بدون وابستگی به نام Delos ذخیره می‌کند. بنابراین، زنجیره استفاده مجدد از اجزا به این شکل است: Tectonic → ZippyDB → Shard Manager → ServiceRouter → Delos. تمام این اجزای قابل استفاده مجدد، برای بسیاری از موارد استفاده دیگر نیز به کار می‌روند. در مقابل، HDFS، یکی از محبوب‌ترین سیستم‌های فایل توزیع‌شده متن‌باز، به‌صورت یکپارچه(monolithic) طراحی شده و تمامی این مؤلفه‌ها را درون خود مدیریت می‌کند.[که می‌تواند انعطاف‌پذیری و مقیاس‌پذیری را محدود کند.]</p>
<h2 id="مطالعه-موردی-فرهنگ-برنامه-threads">مطالعه موردی فرهنگ: برنامه Threads<a hidden class="anchor" aria-hidden="true" href="#مطالعه-موردی-فرهنگ-برنامه-threads">#</a></h2>
<p>توسعه برنامه Threads، که اغلب با توییتر/X مقایسه می‌شود، نمونه‌ای از فرهنگ ذکر شده است. با تأکید بر حرکت سریع، یک تیم کوچک Threads را تنها در پنج ماه کار فنی در محیطی شبیه به استارت‌آپ توسعه داد. علاوه بر این، پس از توسعه، تیم‌های زیرساخت تنها دو روز فرصت داشتند تا برای راه‌اندازی آن در محیط عملیاتی آماده شوند. بیشتر سازمان‌های بزرگ بیش از دو روز زمان صرف می‌کنند تا فقط [طرح] یک برنامه‌کاربردی را که شامل ده‌ها تیم وابسته به هم است را تدوین کنند، چه برسد به اجرای آن. اما در متا، ما به سرعت اتاق‌های جنگ (War Rooms) را در سایت‌های توزیع‌شده ایجاد کردیم و تیم‌های زیرساخت و محصول را گرد هم آوردیم تا مسائل را به صورت بلادرنگ حل‌وفصل کنند. با وجود زمان‌بندی فشرده، راه‌اندازی این برنامه بسیار موفقیت‌آمیز بود و تنها در پنج روز به 100 میلیون کاربر رسید، که آن را به سریع‌ترین برنامه در حال رشد در تاریخ تبدیل کرد.<br>
<strong>زیرساخت مشترک</strong> نقش کلیدی در توانایی تیم‌ها برای پیاده‌سازی سریع Threads و مقیاس‌پذیری قابل اعتماد آن داشت. Threads از بک‌اند پایتون اینستاگرام و همچنین اجزای زیرساخت مشترک متا، مانند پایگاه داده گراف اجتماعی، ذخیره‌سازی کلید-مقدار، پلتفرم سرورلس، پلتفرم‌های آموزش و استنتاج یادگیری ماشین (ML)، و چارچوب مدیریت پیکربندی برای برنامه‌های موبایل، استفاده مجدد کرد.</p>
<p>باز بودن فناوری داخلی متا، با استفاده از مونوریپو، به Threads اجازه داد تا از برخی کدهای برنامه اینستاگرام استفاده مجدد کند و توسعه خود را تسریع بخشد. در مورد باز بودن فناوری خارجی، Threads قصد دارد با ActivityPub، پروتکل شبکه‌سازی اجتماعی متن‌باز، ادغام شود تا با سایر برنامه‌ها سازگاری داشته باشد. ما همچنین تجربیات خود را از توسعه سریع Threads به صورت عمومی به اشتراک گذاشته‌ایم.</p>
<h1 id="جریان-درخواست-کاربر-از-ابتدا-تا-انتها">جریان درخواست کاربر از ابتدا تا انتها<a hidden class="anchor" aria-hidden="true" href="#جریان-درخواست-کاربر-از-ابتدا-تا-انتها">#</a></h1>
<p>اکنون به بررسی فناوری زیرساخت متا می‌پردازیم. محصولات متا توسط یک زیرساخت خدمات مشترک پشتیبانی می‌شوند. برای ارائه یک دیدگاه جامع از این زیرساخت، توضیح می‌دهیم که چگونه یک درخواست کاربر از ابتدا تا انتها پردازش می‌شود و تمام اجزای درگیر را به تفصیل شرح می‌دهیم.</p>
<h2 id="مسیریابی-درخواست">مسیریابی درخواست<a hidden class="anchor" aria-hidden="true" href="#مسیریابی-درخواست">#</a></h2>
<h3 id="نگاشت-دینامیک-dns">نگاشت دینامیک DNS<a hidden class="anchor" aria-hidden="true" href="#نگاشت-دینامیک-dns">#</a></h3>
<p>هنگامی که کاربر درخواستی به facebook.com ارسال می‌کند، سرور DNS متا به صورت دینامیک یک آدرس IP برمی‌گرداند که به یک مرکز داده کوچک لبه (Edge Datacenter) که توسط متا اداره می‌شود و به عنوان نقطه حضور (Point of Presence یا PoP) شناخته می‌شود، نگاشت شده است (همانطور که در شکل ۱ نشان داده شده است). این نگاشت دینامیک DNS اطمینان می‌دهد که PoP انتخاب شده به کاربر نزدیک باشد و در عین حال بار را بین PoPها متعادل کند. اتصال TCP کاربر در PoP خاتمه می‌یابد، که اتصالات TCP جداگانه و طولانی‌مدتی با مراکز داده متا حفظ می‌کند. این تنظیم TCP تقسیم‌شده (Split-TCP) مزایای متعددی دارد، از جمله کاهش تأخیر ایجاد TCP از طریق استفاده مجدد از اتصالات از پیش برقرار شده بین PoPها و مراکز داده. یک PoP معمولاً صدها سرور دارد اما ممکن است تا چند هزار سرور نیز داشته باشد. صدها PoP در سراسر جهان قرار گرفته‌اند تا اطمینان حاصل شود که بیشتر کاربران یک PoP نزدیک به خود دارند، و در نتیجه تأخیر شبکه کوتاه‌مدت را تضمین کنند.</p>
<h3 id="کش-محتوای-استاتیک">کش محتوای استاتیک<a hidden class="anchor" aria-hidden="true" href="#کش-محتوای-استاتیک">#</a></h3>
<p>اگر درخواست کاربر برای محتوای استاتیک باشد، مانند تصاویر و ویدیوها، در صورتی که محتوا از قبل در PoP کش شده باشد، می‌تواند مستقیماً از PoP ارائه شود. علاوه بر این، محتوای استاتیک ممکن است توسط شبکه تحویل محتوا (CDN) نیز کش شود، همانطور که در شکل ۱ نشان داده شده است. هنگامی که حجم قابل توجهی از ترافیک محصولات متا از شبکه یک ارائه‌دهنده خدمات اینترنتی (ISP) سرچشمه می‌گیرد، متا به دنبال ایجاد یک مشارکت دوجانبه سودمند با ارائه تجهیزات شبکه متا (Meta Network Appliances) است که در شبکه ISP میزبانی می‌شوند تا محتوای استاتیک را کش کنند، و در نتیجه یک سایت CDN تشکیل دهند. یک سایت CDN معمولاً ده‌ها سرور دارد و برخی از آن‌ها بیش از صد سرور دارند. هزاران سایت CDN در سراسر جهان، شبکه CDN ما را برای توزیع محتوای استاتیک تشکیل می‌دهند.</p>
<p>محصولات متا از بازنویسی URL برای هدایت درخواست‌های کاربر به یک سایت CDN نزدیک استفاده می‌کنند. هنگامی که یک محصول متا یک URL برای دسترسی کاربر به محتوای استاتیک ارائه می‌دهد، URL را بازنویسی می‌کند، به عنوان مثال از facebook.com/image.jpg به CDN109.meta.com/image.jpg. اگر تصویر در CDN109 کش نشده باشد و کاربر آن را درخواست کند، CDN109 درخواست را به یک PoP نزدیک ارسال می‌کند. سپس PoP درخواست را به متعادل‌کننده بار (Load Balancer) در یک منطقه مرکز داده ارسال می‌کند، که تصویر را از سیستم ذخیره‌سازی بازیابی می‌کند. در مسیر بازگشت، هم PoP و هم سایت CDN تصویر را برای استفاده‌های بعدی کش می‌کنند.</p>
<h3 id="مسیریابی-درخواست-محتواهای-پویا">مسیریابی درخواست محتواهای پویا<a hidden class="anchor" aria-hidden="true" href="#مسیریابی-درخواست-محتواهای-پویا">#</a></h3>
<p>اگر درخواست کاربر برای محتوای دینامیک مانند فید خبری باشد، PoP آن را به یک منطقه مرکز داده ارسال می‌کند. انتخاب منطقه هدف توسط یک ابزار مهندسی ترافیک هدایت می‌شود که به طور دوره‌ای توزیع بهینه ترافیک جهانی از PoPها به مراکز داده را با در نظر گرفتن عواملی مانند ظرفیت مرکز داده و تأخیر شبکه محاسبه می‌کند.</p>
<p>ترافیک PoP به مرکز داده از طریق شبکه گسترده خصوصی (WAN) متا منتقل می‌شود، که PoPها و مراکز داده متا را در سطح جهانی با استفاده از فیبرهای نوری به طول ده‌ها هزار مایل به هم متصل می‌کند. ترافیک شبکه داخلی بین مراکز داده و PoPهای ما به طور قابل توجهی از ترافیک خارجی بین کاربران و PoPها بیشتر است، که این امر عمدتاً به دلیل تکثیر داده‌ها در مراکز داده و تعاملات بین سرویس‌های ریز (Microservices) ما است. شبکه WAN خصوصی پهنای باند بالایی را برای خدمت‌رسانی به این ترافیک داخلی فراهم می‌کند.</p>
<figure>
    <img loading="lazy" src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig01.jpg"
         alt="My image"/> <figcaption>
            <p>شکل۱-زیرساخت جهانی متا!</p>
        </figcaption>
</figure>

<h2 id="توپولوژی-زیرساخت">توپولوژی زیرساخت<a hidden class="anchor" aria-hidden="true" href="#توپولوژی-زیرساخت">#</a></h2>
<p>جدول ۱ اجزای زیرساختی که پیش‌تر ذکر شد را خلاصه می‌کند. در سطح جهانی، ده‌ها منطقه مرکز داده، صدها مرکز داده لبه (PoPها) و هزاران سایت CDN وجود دارد. هر منطقه مرکز داده شامل چندین مرکز داده است که در محدوده چند مایلی یکدیگر قرار دارند. هر مرکز داده از حداکثر یک دوجین تابلوهای اصلی سوئیچ (MSB) برای توزیع برق استفاده می‌کند، که به عنوان دامنه‌های خطای زیر-مرکز داده اصلی نیز عمل می‌کنند. خرابی یک MSB می‌تواند باعث از کار افتادن ۱۰ تا ۲۰ هزار سرور شود.</p>
<h3 id="شبکه-لبه-edge-network">شبکه لبه (Edge Network)<a hidden class="anchor" aria-hidden="true" href="#شبکه-لبه-edge-network">#</a></h3>
<p>یک PoP به چندین سیستم مستقل (Autonomous Systems) در اینترنت متصل است و معمولاً چندین مسیر برای دسترسی به شبکه کاربر دارد. هنگام انتخاب مسیر بین یک PoP و یک کاربر، پروتکل Border Gateway Protocol (BGP) به طور پیش‌فرض ظرفیت و عملکرد شبکه را در نظر نمی‌گیرد. با این حال، شبکه PoP این عوامل را در نظر می‌گیرد و مسیر ترجیحی خود را به یک پیشوند شبکه (Network Prefix) اعلام می‌کند.</p>
<h3 id="شبکه-مرکز-داده-datacenter-network">شبکه مرکز داده (Datacenter Network)<a hidden class="anchor" aria-hidden="true" href="#شبکه-مرکز-داده-datacenter-network">#</a></h3>
<p>سرورها در یک مرکز داده توسط یک شبکه داخلی مرکز داده (Datacenter Fabric) به هم متصل می‌شوند، جایی که سوئیچ‌های شبکه یک توپولوژی سه‌سطحی Clos تشکیل می‌دهند که می‌تواند به صورت افزایشی با افزودن سوئیچ‌های بیشتر در سطح بالایی مقیاس‌پذیر شود. با تعداد کافی سوئیچ‌های سطح بالا، این شبکه می‌تواند یک شبکه بدون انسداد (Non-Blocking) و بدون اشباع (Non-Oversubscribed) ارائه دهد که امکان ارتباط بین هر دو سرور با حداکثر پهنای باند NIC را فراهم می‌کند. ما در حال حرکت به سمت حذف اشباع شبکه (Network Oversubscription) در داخل یک مرکز داده هستیم.</p>
<h3 id="شبکه-منطقهای-regional-network">شبکه منطقه‌ای (Regional Network)<a hidden class="anchor" aria-hidden="true" href="#شبکه-منطقهای-regional-network">#</a></h3>
<p>یک تجمیع‌کننده شبکه (Fabric Aggregator) مراکز داده را در یک منطقه به هم متصل کرده و آن‌ها را به شبکه WAN خصوصی ما متصل می‌کند. تجمیع‌کننده شبکه از یک توپولوژی شبیه به درخت چاق (Fat-Tree) استفاده می‌کند که امکان افزودن سوئیچ‌های بیشتر برای افزایش پهنای باند را فراهم می‌کند. ما هدفمان این است که اشباع شبکه در یک منطقه را به طور قابل توجهی کاهش دهیم تا ارتباط بین مراکز داده درون یک منطقه به یک گلوگاه تبدیل نشود. این امر به اکثر سرویس‌ها (به جز آموزش یادگیری ماشین) اجازه می‌دهد تا در مراکز داده یک منطقه پراکنده شوند بدون اینکه نگران کاهش قابل توجه عملکرد باشند.</p>


    
    
    
    
    
    
    
    
    

    
    
    
    
    

    <div class="data-table" role="region" tabindex="0" aria-labelledby="table-caption-t-478136295">
    <table class="table " id="t-478136295" itemscope itemtype="https://schema.org/Table"><caption id="table-caption-t-478136295" itemprop="about">جدول۱-تعداد و اندازه‌ی اجزای زیرساخت متا</caption>
<thead>
<tr>
<th>نوع زیرساخت</th>
<th style="text-align:center">تعداد</th>
<th style="text-align:center">تعداد سرور در هر زیرساخت</th>
</tr>
</thead>
<tbody>
<tr>
<td>منطقه‌ای</td>
<td style="text-align:center">O(10)</td>
<td style="text-align:center">یک میلیون</td>
</tr>
<tr>
<td>PoP</td>
<td style="text-align:center">O(100)</td>
<td style="text-align:center">از ده ها تا هزاران</td>
</tr>
<tr>
<td>سایت CDN</td>
<td style="text-align:center">O(1,000)</td>
<td style="text-align:center">ده‌ها تا بیش از صد</td>
</tr>
<tr>
<td>دیتاسنتر</td>
<td style="text-align:center">چندین دیتاسنتر در هر منطقه</td>
<td style="text-align:center">صدهزاران</td>
</tr>
<tr>
<td>MSB</td>
<td style="text-align:center">ده‌ها MSB در هر دیتاسنتر</td>
<td style="text-align:center">بین ۱۰ هزار تا ۲۰ هزار</td>
</tr>
</tbody>
</table>

    </div>
    
    
    
<h2 id="پردازش-درخواست">پردازش درخواست<a hidden class="anchor" aria-hidden="true" href="#پردازش-درخواست">#</a></h2>
<h3 id="پردازش-آنلاین">پردازش آنلاین<a hidden class="anchor" aria-hidden="true" href="#پردازش-آنلاین">#</a></h3>
<p>هنگامی که یک درخواست کاربر به یک منطقه مرکز داده می‌رسد، در طول مسیری که در شکل ۲ نشان داده شده است، پردازش می‌شود. متعادل‌کننده بار (Load Balancer) درخواست‌های کاربر را بین ده‌ها هزار سرور توزیع می‌کند که &ldquo;توابع سرورلس فرانت‌اند&rdquo; را اجرا می‌کنند. برای پردازش یک درخواست کاربر، یک تابع سرورلس فرانت‌اند ممکن است بسیاری از سرویس‌های بک‌اند را فراخوانی کند، که برخی از آن‌ها ممکن است به &ldquo;استنتاج یادگیری ماشین (ML Inference)&rdquo; نیز متکی باشند، مثلاً برای بازیابی توصیه‌های تبلیغات یا محتوای فید خبری. <br>
در طول اجرا، یک تابع سرورلس فرانت‌اند می‌تواند رویدادهایی را در &ldquo;صف رویداد (Event Queue)&rdquo; قرار دهد تا &ldquo;توابع سرورلس رویداد-محور (Event-Driven Serverless Functions)&rdquo; به صورت ناهمزمان آن‌ها را پردازش کنند. یکی از این رویدادها می‌تواند ارسال یک ایمیل تأیید پس از انجام یک عمل توسط کاربر در سایت باشد. در حالی که توابع سرورلس فرانت‌اند مستقیماً بر زمان پاسخ درک‌شده توسط کاربر تأثیر می‌گذارند و بنابراین دارای یک هدف سطح سرویس (SLO) سخت‌گیرانه برای تأخیر هستند، توابع سرورلس رویداد-محور به صورت ناهمزمان کار می‌کنند و بر زمان پاسخ درک‌شده توسط کاربر تأثیری ندارند، و برای توان عملیاتی (Throughput) و بهره‌وری سخت‌افزاری بهینه‌سازی شده‌اند، نه تأخیر. نسبت سرورهایی که توابع سرورلس فرانت‌اند را اجرا می‌کنند به سرورهایی که توابع سرورلس رویداد-محور را اجرا می‌کنند، تقریباً ۵ به ۱ است.</p>
<h3 id="پردازش-آفلاین">پردازش آفلاین<a hidden class="anchor" aria-hidden="true" href="#پردازش-آفلاین">#</a></h3>
<p>اجزای سمت راست شکل ۲ پردازش‌های آفلاین مختلفی را انجام می‌دهند تا به پردازش آنلاین در سمت چپ کمک کنند. جداسازی پردازش آنلاین و آفلاین امکان بهینه‌سازی مستقل بر اساس ویژگی‌های بار کاری هر یک را فراهم می‌کند. هنگام پردازش درخواست‌های کاربر، توابع سرورلس فرانت‌اند و سرویس‌های بک‌اند انواع مختلفی از داده‌ها، مانند معیارهای کلیک روی تبلیغات یا تماشای ویدیو، را در &ldquo;انبار داده (Data Warehouse)&rdquo; ثبت می‌کنند. این داده‌ها به عنوان ورودی برای پردازش‌های آفلاین مختلف استفاده می‌شوند. به عنوان مثال، &ldquo;آموزش یادگیری ماشین (ML Training)&rdquo; از این داده‌ها برای به‌روزرسانی مدل‌های یادگیری ماشین استفاده می‌کند، در حالی که &ldquo;پردازش جریان (Stream Processing)&rdquo; می‌تواند از این داده‌ها برای به‌روزرسانی موضوعات داغ سایت و ذخیره آن‌ها در &ldquo;پایگاه‌های داده و کش‌ها (Databases and Caches)&rdquo; استفاده کند، که سپس در طول پردازش درخواست‌های آنلاین کاربر مورد استفاده قرار می‌گیرند. علاوه بر این، &ldquo;تحلیل دسته‌ای (Batch Analytics)&rdquo; که توسط Spark و Presto پشتیبانی می‌شود، می‌تواند به طور دوره‌ای عملیاتی مانند به‌روزرسانی توصیه‌های دوستان در پاسخ به فعالیت‌های جدید در سایت را انجام دهد. در نهایت، به‌روزرسانی‌های داده در انبار داده به عنوان منبع اصلی رویداد عمل می‌کند که اجرای توابع سرورلس رویداد-محور را فعال می‌کند.</p>
<figure>
    <img loading="lazy" src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig02.jpg"
         alt="My image"/> <figcaption>
            <p>شکل۲-معماری کلی اجزای نرم‌افزاری که در یک دیتاسنتر منطقه‌ای اجرا می‌شوند. این دیاگرام به‌شدت ساده‌سازی شده است، زیرا متا به‌صورت داخلی بیش از ۱۰,۰۰۰ سرویس بک‌اند دارد که دارای یک گراف فراخوانی پیچیده هستند.</p>
        </figcaption>
</figure>

<h1 id="افزایش-بهرهوری-توسعهدهندگان">افزایش بهره‌وری توسعه‌دهندگان<a hidden class="anchor" aria-hidden="true" href="#افزایش-بهرهوری-توسعهدهندگان">#</a></h1>
<p>هدف اصلی یک زیرساخت مشترک، افزایش بهره‌وری توسعه‌دهندگان است. در حالی که به طور گسترده‌ای پذیرفته شده است که استقرار مداوم نرم‌افزار (continuous software deployment) و توابع سرورلس (serverless) می‌توانند به افزایش بهره‌وری توسعه‌دهندگان کمک کنند، ما این رویکردها را به حد افراط رسانده‌ایم.</p>
<h2 id="استقرار-مداوم">استقرار مداوم<a hidden class="anchor" aria-hidden="true" href="#استقرار-مداوم">#</a></h2>
<p>همسو با فرهنگ سرعت بالا، ما استقرار مداوم کد و پیکربندی را با سرعت و مقیاس بسیار بالا انجام می‌دهیم. این رویکرد به توسعه‌دهندگان امکان می‌دهد تا ویژگی‌های جدید و اصلاحات را به‌سرعت منتشر کنند، بازخورد فوری دریافت کنند و به‌صورت سریع و مداوم بهبودهای لازم را اعمال کنند.<br>
برای تغییرات پیکربندی، ابزار مدیریت پیکربندی ما روزانه بیش از ۱۰۰,۰۰۰ تغییر زنده را در محیط تولید اجرا می‌کند که شامل O(10,000) سرویس و میلیون‌ها سرور می‌شود. این تغییرات وظایف مختلفی را تسهیل می‌کنند، از جمله توازن بار، عرضه ویژگی‌ها، تست‌های A/B و محافظت در برابر اضافه‌بار. در متا، تقریباً هر مهندسی که کد می‌نویسد، تغییرات پیکربندی زنده را نیز در محیط تولید اعمال می‌کند. با پیروی از پارادایم &ldquo;پیکربندی به عنوان کد&rdquo;، تغییرات دستی پیکربندی قبل از ثبت در مخزن کد، توسط هم‌تیمی‌ها بررسی می‌شوند. پس از ثبت، این تغییرات بلافاصله وارد خط لوله استقرار مداوم می‌شوند. در عرض چند ثانیه، پیکربندی به‌روزرسانی‌شده می‌تواند به میلیون‌ها فرآیند لینوکس مشترک ارسال شود و یک اعلان upcall را فعال کند. فرآیندها می‌توانند بلافاصله رفتار زمان اجرای خود را بدون نیاز به راه‌اندازی مجدد تنظیم کنند. علاوه بر تغییرات دستی، ابزارهای خودکار نیز تغییرات پیکربندی را هدایت می‌کنند، مثلاً برای توازن بار.</p>
<p>برای تغییرات کد، ابزار استقرار ما بیش از ۳۰,۰۰۰ خط لوله را برای استقرار ارتقاء نرم‌افزار مدیریت می‌کند. در متا، ۹۷٪ از سرویس‌ها از استقرار کاملاً خودکار نرم‌افزار بدون هیچ گونه مداخله دستی استفاده می‌کنند: ۵۵٪ از استقرار مداوم استفاده می‌کنند و هر تغییر کد را بلافاصله پس از گذراندن تست‌های خودکار به محیط تولید منتشر می‌کنند، در حالی که ۴۲٪ باقی‌مانده به طور خودکار در یک برنامه ثابت، معمولاً روزانه یا هفتگی، مستقر می‌شوند. به عنوان مثال، توابع سرورلس frontend در شکل ۲ را در نظر بگیرید. این توابع بر روی بیش از نیم میلیون سرور اجرا می‌شوند و بیش از ۱۰,۰۰۰ توسعه‌دهنده محصول هر روز کاری کد آن‌ها را تغییر می‌دهند و هزاران commit کد انجام می‌شود. با وجود این محیط بسیار پویا، هر سه ساعت یک بار نسخه جدیدی از تمام توابع سرورلس به محیط تولید منتشر می‌شود.</p>
<p>حتی نرم‌افزار شبکه ما نیز مانند سرویس‌های معمولی طراحی شده و برای به‌روزرسانی‌های مکرر بهینه‌سازی شده است. به عنوان مثال، شبکه خصوصی WAN ما توپولوژی شبکه را به چندین صفحه موازی تقسیم می‌کند که هر کدام مسئول بخشی از ترافیک هستند و کنترلر خود را دارند. این امر امکان به‌روزرسانی‌های مکرر نرم‌افزار کنترلر را فراهم می‌کند. توسعه‌دهندگان می‌توانند با هدایت ترافیک از یک صفحه و استقرار الگوریتم جدید فقط در آن صفحه، بدون تأثیر بر سایر صفحات، الگوریتم‌های کنترل جدید را آزمایش کنند. به طور مشابه، نرم‌افزار سوئیچ شبکه ما نیز مانند سرویس‌های استاندارد به‌روزرسانی‌های مکرر را انجام می‌دهد. با استفاده از ویژگی &ldquo;warm boot&rdquo; در ASIC سوئیچ، صفحه داده به انتقال ترافیک ادامه می‌دهد در حالی که نرم‌افزار سوئیچ در حال به‌روزرسانی است.</p>
<p>به‌روزرسانی‌های مکرر کد و پیکربندی، توسعه چابک نرم‌افزار را ممکن می‌سازد اما خطر قطعی سایت را افزایش می‌دهد. برای مقابله با این خطر، ما سرمایه‌گذاری زیادی در تست‌ها، عرضه‌های مرحله‌ای و بررسی‌های سلامت در طول به‌روزرسانی‌ها انجام داده‌ایم. پیش‌تر، یک کمپین شرکت‌محور برای افزایش خودکارسازی استقرار کد راه‌اندازی کردیم که باعث شد پذیرش استقرار کاملاً خودکار کد محافظت‌شده توسط بررسی‌های سلامت از ۱۲٪ به ۹۷٪ افزایش یابد. به طور مشابه، ابتکار دیگری را اجرا کردیم تا اطمینان حاصل کنیم که تمام تغییرات پیکربندی تحت تست‌های خودکار canary قرار می‌گیرند تا ایمنی پیکربندی حفظ شود. به طور کلی، ما این سرمایه‌گذاری‌ها در استقرار مداوم را ارزشمند می‌دانیم، زیرا به طور قابل توجهی بهره‌وری توسعه‌دهندگان را افزایش می‌دهد.</p>
<h2 id="توابع-سرورلس-serverless-functions">توابع سرورلس (Serverless functions):<a hidden class="anchor" aria-hidden="true" href="#توابع-سرورلس-serverless-functions">#</a></h2>
<p>استفاده گسترده از توابع سرورلس (که به عنوان Function-as-a-Service یا FaaS نیز شناخته می‌شوند) یکی دیگر از عوامل کلیدی است که بهره‌وری توسعه‌دهندگان را افزایش می‌دهد. برخلاف سرویس‌های بک‌اند سنتی که می‌توانند پیچیدگی‌های زیادی داشته باشند، FaaS بدون حالت (stateless) است و یک رابط(interface) ساده برای اجرای توابع ارائه می‌دهد. هر فراخوانی FaaS به طور مستقل مدیریت می‌شود و هیچ اثر جانبی بر فراخوانی‌های همزمان دیگر ندارد، مگر از طریق حالت‌هایی که در پایگاه‌های داده خارجی ذخیره شده‌اند. به دلیل ماهیت بدون حالت FaaS، این سیستم به شدت به سیستم‌های کش خارجی متکی است تا در هنگام دسترسی به پایگاه‌های داده، عملکرد خوبی را ارائه دهد.</p>
<p>توسعه‌دهندگان کد FaaS را می‌نویسند و بقیه کارها را به زیرساخت می‌سپارند تا از طریق اتوماسیون انجام شود، از جمله استقرار کد و مقیاس‌دهی خودکار در پاسخ به تغییرات بار. این سادگی به بیش از ۱۰,۰۰۰ توسعه‌دهنده محصول متا اجازه می‌دهد تا تنها بر روی منطق محصول تمرکز کنند و نگران مدیریت زیرساخت نباشند. علاوه بر این، این رویکرد از هدررفت منابع سخت‌افزاری ناشی از تأمین بیش از حد منابع توسط توسعه‌دهندگان محصول جلوگیری می‌کند.</p>
<p>متا استفاده از FaaS را به حد نهایی رسانده تا بهره‌وری توسعه‌دهندگان را به حداکثر برساند. از بین حدود ۱۰,۰۰۰ مهندس در متا، تعداد مهندسانی که کد FaaS می‌نویسند حدود ۵۰٪ بیشتر از کسانی است که کد برای سرویس‌های معمولی که خودشان مدیریت می‌کنند می‌نویسند. این موفقیت نه تنها به دلیل خلاصی مهندسان محصول از[شرّ] مدیریت زیرساخت، بلکه به دلیل قابلیت استفاده محیط توسعه یکپارچه (IDE) برای FaaS است. این IDE دسترسی آسان به پایگاه‌ داده گراف اجتماعی و سیستم‌های بک‌اند مختلف را از طریق ساختارهای زبان سطح بالا فراهم می‌کند. همچنین بازخورد سریع را از طریق تست‌های یکپارچه‌سازی مداوم ارائه می‌دهد.</p>
<p>همان‌طور که در شکل ۲ نشان داده شده است، متا دو پلتفرم FaaS را اداره می‌کند: یکی برای «توابع سرورلس فرانت‌اند» و دیگری برای «توابع سرورلس رویداد-محور». ما به ترتیب به آن‌ها FrontFaaS و XFaaS می‌گوییم. توابع FrontFaaS در PHP نوشته می‌شوند (ما پلتفرم‌های FaaS برای توابع پایتون، ارلنگ و Haskell نیز داریم). برای پشتیبانی از بار بالای تولید شده توسط میلیاردها کاربر، بیش از نیم میلیون سرور را نگهداری می‌کنیم که زمان اجرای PHP را همیشه فعال نگه می‌دارند. هنگامی که یک درخواست کاربر می‌رسد، به یکی از این سرورها هدایت می‌شود تا بلافاصله پردازش شود، بدون اینکه زمان شروع سرد (cold start) را تجربه کند. هنگامی که بار سایت کم است، از مقیاس‌دهی خودکار استفاده می‌کنیم تا برخی از سرورهای FrontFaaS را برای استفاده سایر سرویس‌ها آزاد کنیم.</p>
<p>XFaaS شباهت‌های زیادی با FrontFaaS دارد، با این تفاوت کلیدی که توابعی را اجرا می‌کند که به کاربر نهایی مربوط نمی‌شوند و نیازی به زمان پاسخ کمتر از یک ثانیه ندارند، اما الگوی بار بسیار ناگهانی (spiky) دارند. برای جلوگیری از تحمیل بیش‌ از حد بار روی زیرساخت‌ها، XFaaS از ترکیبی از بهینه‌سازی‌ها برای گسترش اجرای توابع استفاده می‌کند، از جمله به تعویق انداختن اجرای توابع تحمل‌پذیر، تأخیر به ساعات کم‌بار، متعادل‌سازی بار جهانی فراخوانی‌های توابع در مناطق مختلف و اعمال محدودیت بر اساس سهمیه‌ها.</p>
<p>توسعه‌دهندگان محصول در متا از اواخر دهه ۲۰۰۰ از FaaS به عنوان پارادایم اصلی کدنویسی خود استفاده کرده‌اند، حتی قبل از اینکه اصطلاح FaaS رایج شود. در مقایسه با پلتفرم‌های سرورلس در صنعت، یک جنبه منحصر به فرد پلتفرم‌های سرورلس ما این است که به چندین تابع اجازه می‌دهند به طور همزمان در یک فرآیند لینوکس اجرا شوند تا کارایی سخت‌افزاری بالاتری داشته باشند، برخلاف ابرهای عمومی که برای اطمینان از جداسازی قوی‌تر بین مشتریان مختلف، مجبورند یک تابع را در هر ماشین مجازی اجرا کنند.</p>
<h1 id="کاهش-هزینههای-سختافزاری">کاهش هزینه‌های سخت‌افزاری<a hidden class="anchor" aria-hidden="true" href="#کاهش-هزینههای-سختافزاری">#</a></h1>
<p>علاوه بر افزایش بهره‌وری توسعه‌دهندگان، هدف اصلی دیگر یک زیرساخت مشترک، کاهش هزینه‌های سخت‌افزاری است. در این بخش، به چند نمونه از نحوه کمک راه‌حل‌های نرم‌افزاری به کاهش هزینه‌های سخت‌افزاری اشاره می‌کنیم.</p>
<p><strong>تمامی دیتاسنترهای جهانی به‌عنوان یک رایانه واحد</strong> <br>
بیشتر ارائه دهندگان زیرساخت‌، مدیریت پیچیدگی‌های مربوط به دیتاسنترهای توزیع‌شده جغرافیایی را بر عهده‌ی کاربران می‌گذارند. کاربران باید به‌صورت دستی تعداد نسخه‌های کپی (replicas) سرویس‌های خود را تعیین کرده و مناطق مناسب برای استقرار را انتخاب کنند، درحالی‌که همچنان باید الزامات سطح سرویس (SLO) را رعایت کنند. این پیچیدگی اغلب منجر به هدررفت سخت‌افزار به دلیل تخصیص بیش از حد منابع، توزیع نامتعادل بار بین مناطق مختلف، و مهاجرت ناکافی سرویس‌ها بین مناطق برای انطباق با تغییرات در میزان تقاضای پردازشی و ظرفیت دیتاسنترها می‌شود.</p>
<p>در مقابل، متا در حال حرکت از رویکرد &ldquo;دیتاسنتر به عنوان یک کامپیوتر&rdquo; (DaaC) به سمت چشم‌انداز &ldquo;تمامی دیتاسنترها در (سطح) جهان به عنوان یک کامپیوتر&rdquo; (Global-DaaC) است. با Global-DaaC، کاربران به سادگی درخواست استقرار جهانی یک سرویس را می‌دهند و زیرساخت تمام جزئیات را مدیریت می‌کند: تعیین تعداد بهینه رپلیکاهای سرویس، قرار دادن این رپلیکاها در دیتاسنترهای منطقه‌ای بر اساس اهداف سطح سرویس و سخت‌افزار موجود، انتخاب نوع سخت‌افزار مناسب‌ترین، بهینه‌سازی مسیریابی ترافیک و تطبیق مداوم جایگاه سرویس در پاسخ به تغییرات بار کاری. در مقایسه با ابرهای عمومی، متا می‌تواند Global-DaaC را راحت‌تر محقق کند زیرا مالک تمام برنامه‌های خود است و می‌تواند آن‌ها را در صورت نیاز بین مناطق جابجا کند؛ در حالی که ابرهای عمومی این انعطاف‌پذیری را با برنامه‌های مشتریان خود ندارند.</p>
<p>برای پیاده‌سازی Global-DaaC، ابزارهای ما به طور یکپارچه تخصیص منابع را در تمام سطوح هماهنگ می‌کنند: جهانی، منطقه‌ای و سرورهای انحصاری. ابتدا، <a href="https://cacm.acm.org/research/metas-hyperscale-infrastructure-overview-and-insights/#B13">ابزار مدیریت ظرفیت جهانی</a> ما با استفاده از ردیابی RPC وابستگی‌های سرویس را شناسایی کرده و مدل‌های مصرف منابع را می‌سازد، سپس از برنامه‌ریزی عدد صحیح مختلط (Mixed-Integer Programming) برای تقسیم نیازهای ظرفیت جهانی یک سرویس به سهمیه‌های منطقه‌ای استفاده می‌کند. در مرحله بعد، ابزار مدیریت ظرفیت منطقه‌ای ما منابع سرور را به این سهمیه‌های منطقه‌ای اختصاص می‌دهد تا خوشه‌های مجازی (Virtual Clusters) تشکیل دهد. برخلاف خوشه‌های فیزیکی، یک خوشه مجازی می‌تواند شامل سرورهایی از مراکز داده مختلف در یک منطقه باشد و اندازه آن می‌تواند به صورت پویا افزایش یا کاهش یابد. در زمان اجرا، ابزار مدیریت کانتینر ما کانتینرها را در این خوشه‌های مجازی تخصیص می‌دهد، که اغلب کانتینرهای یک کار را در چندین مرکز داده در یک منطقه پخش می‌کند تا تحمل خطا (Fault Tolerance) بهبود یابد. در نهایت، در سطح سرور، مکانیزم‌های کرنل ما اطمینان حاصل می‌کنند که حافظه و منابع I/O اختصاص داده شده به کانتینرهای فردی به درستی به اشتراک گذاشته شده و ایزوله شوند.</p>
<p>سرویس‌های stateful، مانند پایگاه‌های داده از Global-DaaC بهره می‌برند. این سرویس‌ها معمولاً به‌صورت sharded اجرا می‌شوند، به این معنا که هر کانتینر برای افزایش بهره‌وری، چندین بخش داده (shard) را میزبانی می‌کند.  Global Service Placer(GSP) ما از الگوریتم‌های بهینه‌سازی با در نظر گرفتن محدودیت‌ها برای تعیین تعداد بهینه‌ی نسخه‌های کپی (replicas) برای هر بخش داده و <strong>توزیع آن‌ها در مناطق مختلف</strong> استفاده می‌کند. سپس، <strong>چارچوب شاردینگ</strong> ما در محدوده‌ی این قوانین عمل کرده و نسخه‌های داده را بین کانتینرها تخصیص می‌دهد و آن‌ها را <strong>به‌صورت پویا جابه‌جا می‌کند</strong> تا با تغییرات بار پردازشی سازگار شود.</p>
<p>به طور مشابه، بارکاری یادگیری ماشین (ML) نیز از Global-DaaC بهره می‌برند. برای استنتاج ML، مدل‌ها مشابه shardهای داده مدیریت می‌شوند، و تعداد رپلیکاهای مدل و مکان آن‌ها توسط GSP تعیین می‌شود. برای آموزش ML، نیاز به هم‌مکانی (Collocation) داده‌های آموزشی و GPUها در یک منطقه مرکز داده است. هر تیم یک سهمیه ظرفیت جهانی GPU دریافت می‌کند و کارهای آموزشی را به یک صف کار جهانی ارسال می‌کند. زمان‌بند آموزش ML ما به طور خودکار مناطق را برای تکثیر داده و تخصیص GPU انتخاب می‌کند تا هم‌مکانی داده و GPUها را تضمین کند و در عین حال بهره‌وری GPU را به حداکثر برساند.</p>
<h2 id="طراحی-مشارکتی-سختافزار-و-نرمافزار">طراحی مشارکتی سخت‌افزار و نرم‌افزار<a hidden class="anchor" aria-hidden="true" href="#طراحی-مشارکتی-سختافزار-و-نرمافزار">#</a></h2>
<p>در حالی که طراحی مشارکتی سخت‌افزار و نرم‌افزار در سطح یک سرور واحد رایج است، ما این مفهوم را به مقیاس جهانی ارتقا داده‌ایم تا با استفاده از راه‌حل‌های نرم‌افزاری، محدودیت‌های سخت‌افزاری را با هزینه‌ی کمتر برطرف کنیم.</p>
<h3 id="تحمل-خطای-کمهزینه">تحمل خطای کم‌هزینه<a hidden class="anchor" aria-hidden="true" href="#تحمل-خطای-کمهزینه">#</a></h3>
<p>ابرهای عمومی تمایل دارند سخت‌افزاری با دسترسی بالاتر ارائه دهند، زیرا برنامه‌های مشتریان آن‌ها ممکن است به اندازه کافی در برابر خطا مقاوم نباشند. در مقابل، از آنجا که تمام برنامه‌های ما تحت کنترل ما هستند، می‌توانیم اطمینان حاصل کنیم که آن‌ها به گونه‌ای پیاده‌سازی شده‌اند که در برابر خطا مقاوم باشند و روی سخت‌افزار ارزان‌تر با تضمین‌های دسترسی پایین‌تر اجرا شوند. به عنوان مثال، یک رک سرور در ابرهای عمومی ممکن است از دو منبع تغذیه و دو سوئیچ بالای رک (ToR) استفاده کند تا دسترسی بالا را تضمین کند و تعمیر و نگهداری سوئیچ‌ها بدون اختلال در بارهای کاری در حال اجرا انجام شود. در مقابل، رک‌های ما نه منبع تغذیه دوگانه دارند و نه سوئیچ‌های ToR دوگانه. در عوض، افزونگی سخت‌افزاری تنها در سطح بسیار بزرگ‌تری مانند تابلوهای اصلی سوئیچ (MSB) اتفاق می‌افتد، که هر کدام حدود ۱۰,۰۰۰ تا ۲۰,۰۰۰ سرور را پوشش می‌دهند. برای هر شش MSB، تنها یک MSB رزرو به عنوان پشتیبان وجود دارد. علاوه بر این، ماشین‌های مجازی (VM) در ابرهای عمومی اغلب از دستگاه‌های بلوک متصل به شبکه استفاده می‌کنند، که امکان مهاجرت زنده VM را فراهم می‌کند. در مقابل، کانتینرهای ما از SSDهای متصل مستقیم و کم‌هزینه برای دیسک‌های ریشه استفاده می‌کنند، که مهاجرت زنده کانتینر را در طول عملیات نگهداری مرکز داده دشوار می‌کند.</p>
<p>ما از راه‌حل‌های نرم‌افزاری برای غلبه بر محدودیت‌های سخت‌افزار کم‌هزینه استفاده می‌کنیم. اولاً، ابزارهای تخصیص منابع ما اطمینان می‌دهند که کانتینرها و shardهای داده یک سرویس به اندازه کافی در دامنه‌های خطای زیر-مرکز داده (MSBها) پخش شده‌اند تا تحمل خطا بهبود یابد. ثانیاً، از طریق یک پروتکل همکاری که به یک سرویس اجازه می‌دهد در مدیریت چرخه عمر کانتینرهای خود مشارکت کند، اطمینان حاصل می‌کنیم که عملیات نگهداری محدودیت‌های سطح برنامه را رعایت می‌کنند، مانند جلوگیری از خاموشی همزمان دو رپلیکا از یک shard داده. در نهایت، Global-DaaC اطمینان می‌دهد که سرویس‌ها به گونه‌ای مستقر می‌شوند که بتوانند از دست دادن همزمان یک منطقه کامل مرکز داده، یک MSB در هر منطقه و درصدی از سرورهای تصادفی در هر منطقه را تحمل کنند. ما به طور معمول تست‌هایی در محیط عملیاتی انجام می‌دهیم تا اطمینان حاصل کنیم که این ویژگی‌ها حفظ می‌شوند و سرویس‌های ما در برابر خطا مقاوم هستند.</p>
<p>در حالی که زیرساخت ما به گونه‌ای طراحی شده است که از دست دادن یک منطقه کامل مرکز داده را بدون تأثیر بر کاربران تحمل کند، افزایش تعداد مناطق احتمال تحت تأثیر قرار گرفتن همزمان دو منطقه نزدیک به هم توسط یک فاجعه طبیعی بزرگ، مانند طوفان، را افزایش داده است. به جای تأمین بیش از حد ظرفیت برای تحمل از دست دادن همزمان دو منطقه، ما از یک رویکرد نرم‌افزاری استفاده می‌کنیم که در صورت از دست دادن چندین منطقه، ویژگی‌های کم‌اهمیت‌تر محصول را غیرفعال کرده و کیفیت سرویس را به طور کنترل‌شده کاهش می‌دهد، مانند ارائه ویدیوهای با کیفیت پایین‌تر، تا فشار روی زیرساخت‌ها کاهش یابد.</p>
<h3 id="حذف-هزینههای-پروکسیهای-مسیریابی">حذف هزینه‌های پروکسی‌های مسیریابی<a hidden class="anchor" aria-hidden="true" href="#حذف-هزینههای-پروکسیهای-مسیریابی">#</a></h3>
<p>برخلاف شبکه‌های سرویس سنتی که عمدتاً از پروکسی‌های sidecar برای مسیریابی درخواست‌های RPC استفاده می‌کنند، شبکه سرویس متا از پروکسی‌ها تنها برای مسیریابی ۱٪ از درخواست‌های RPC در ناوگان ما استفاده می‌کند. ۹۹٪ باقی‌مانده از یک کتابخانه مسیریابی استفاده می‌کنند که به اجرایی‌های سرویس لینک شده است و مسیریابی مستقیم از کلاینت به سرور را انجام می‌دهد، بدون نیاز به پروکسی‌های واسطه. در حالی که این رویکرد غیرمعمول باعث صرفه‌جویی در O(100,000) سرور مورد نیاز برای پروکسی‌ها می‌شود، چالش‌های استقرار را به دلیل کامپایل شدن کتابخانه در حدود O(10,000) سرویس، هر کدام با برنامه استقرار خود، افزایش می‌دهد. ابزارهای استقرار نرم‌افزار و مدیریت پیکربندی ما به مدیریت این چالش‌ها کمک می‌کنند.</p>
<h3 id="ذخیرهسازی-لایهای-و-ssdهای-محلی">ذخیره‌سازی لایه‌ای و SSDهای محلی<a hidden class="anchor" aria-hidden="true" href="#ذخیرهسازی-لایهای-و-ssdهای-محلی">#</a></h3>
<p>بر اساس فرکانس دسترسی و تحمل تأخیر، داده‌ها را به سه دسته داغ (Hot)، گرم (Warm) و سرد (Cold) تقسیم می‌کنیم، که هر دسته از یک سیستم ذخیره‌سازی متفاوت برای بهینه‌سازی هزینه-کارایی استفاده می‌کند. پایگاه‌های داده و کش‌های داغ، مانند پایگاه داده گراف اجتماعی، داده‌ها را در حافظه و درایوهای حالت جامد (SSD) ذخیره می‌کنند.</p>
<p>داده‌های گرم، شامل ویدیوها، تصاویر و داده‌های موجود در انبار داده (مانند لاگ‌های فعالیت کاربر)، در یک سیستم فایل توزیع‌شده ذخیره می‌شوند که از درایوهای دیسک سخت (HDD) برای ذخیره داده استفاده می‌کند. هر سرور ذخیره‌سازی مجهز به یک CPU، ۳۶ HDD و دو SSD برای کش متادیتا است.</p>
<p>برای داده‌های سرد که به ندرت به آن‌ها دسترسی می‌شود، مانند یک ویدیوی با وضوح بالا که ده سال پیش ذخیره شده است، آن‌ها را در سرورهای HDD با چگالی بالا آرشیو می‌کنیم، که هر کدام دارای یک CPU و ۲۱۶ HDD هستند و تعادل خوبی بین هزینه کل مالکیت و سرعت بازیابی داده ارائه می‌دهند. این HDDها بیشتر اوقات خاموش هستند، زیرا در حال استفاده فعال نیستند.</p>
<p>در میان بارهای کاری که داده‌ها را روی SSD ذخیره می‌کنند، برخی می‌توانند تأخیرهای طولانی‌تر را تحمل کنند و برای استفاده بهتر از SSD، از ذخیره‌سازی اشتراکی مبتنی بر SSD استفاده می‌کنند. با این حال، بارهای کاری با نیازهای سخت‌گیرانه به تأخیر همچنان از SSDهای محلی متصل مستقیم استفاده می‌کنند. در مقایسه با سایر زیرساخت‌های هایپرسکیل، ما بیشتر از SSDهای محلی برای کاهش هزینه‌ها استفاده می‌کنیم، علیرغم پیچیدگی‌های مدیریتی که به همراه دارد. به عنوان مثال، توزیع نابرابر بار می‌تواند منجر به استفاده ناکافی و به‌دردنخور شدن SSDهای محلی شود. علاوه بر این، بازیابی پس از خطا به دلیل گیر کردن داده‌ها در SSDهای سرورهای خراب، پیچیده می‌شود. برای حل این چالش‌ها، از چارچوب sharding مشترک خود برای پیاده‌سازی سرویس‌های stateful با SSDهای محلی استفاده می‌کنیم، که این مشکلات را یک بار حل کرده و راه‌حل را در بسیاری از سرویس‌ها استفاده مجدد می‌کنیم.</p>
<h2 id="طراحی-اختصاصی-سختافزار">طراحی اختصاصی سخت‌افزار<a hidden class="anchor" aria-hidden="true" href="#طراحی-اختصاصی-سختافزار">#</a></h2>
<p>ما مراکز داده و سخت‌افزارهای خود—از جمله سرورها، سوئیچ‌های شبکه، شتاب‌دهنده‌های ویدیو و تراشه‌های هوش مصنوعی—را برای کاهش هزینه‌ها و بهبود بهره‌وری انرژی خودمان طراحی می‌کنیم. در مراکز داده، تامین برق بزرگ‌ترین محدودیت است، زیرا ظرفیت تأمین برق در زمان ساخت دیتاسنتر مشخص می‌شود و افزایش آن در طول عمر ۲۰ تا ۳۰ ساله‌ی دیتاسنتر بسیار دشوار است. در مقابل، شبکه و سرورها را می‌توان به‌روز کرد. برق در دیتاسنترها معمولاً <strong>بیش از ظرفیت اسمی تخصیص داده می‌شود</strong> (Oversubscribed)، به این معنا که مجموع نیازهای برق سرورها از مقدار واقعی برق قابل تأمین بیشتر در نظر گرفته می‌شود. برای جلوگیری از <strong>مصرف بیش از حد برق</strong> در زمان افزایش ناگهانی بار پردازشی، یک ابزار خودکار اقدامات <strong>محدودسازی مصرف برق</strong> را در سطوح مختلف سیستم تأمین برق هماهنگ می‌کند.</p>
<p>طراحی‌های سخت‌افزاری ما معمولاً از طریق هماهنگی بین سخت‌افزار و نرم‌افزار (مانند بهینه‌سازی استفاده از SRAM در تراشه‌های هوش مصنوعی بر اساس بارهای کاری) و حذف اجزای غیرضروری (مثل حذف سیستم‌های خنک‌کننده با کمپرسور) به صرفه‌جویی در هزینه و انرژی منجر می‌شوند. علاوه بر این، توسعه داخلی سوئیچ‌های شبکه و نرم‌افزارهای مرتبط، این امکان را فراهم می‌کند که نرم‌افزار سوئیچ را به‌عنوان یک سرویس معمولی در نظر گرفته و به‌روزرسانی‌ها را به‌طور مکرر انجام دهیم. بیشتر طراحی‌های سخت‌افزاری ما از طریق پروژه Open Compute به صورت متن‌باز در دسترس هستند.</p>
<h1 id="طراحی-سیستمهای-مقیاسپذیر">طراحی سیستم‌های مقیاس‌پذیر<a hidden class="anchor" aria-hidden="true" href="#طراحی-سیستمهای-مقیاسپذیر">#</a></h1>
<p>یک موضوع تکرارشونده در زیرساخت‌های هایپرسکیل، طراحی سیستم‌های مقیاس‌پذیر است. سیستم‌های غیرمتمرکز طراحی‌شده برای محیط اینترنت، مانند BGP، BitTorrent و جداول هش توزیع‌شده (DHTs)، اغلب به دلیل مقیاس‌پذیری مورد تحسین قرار می‌گیرند. با این حال، در محیط مرکز داده، که محدودیت منابع کمتری دارد و تحت کنترل یک سازمان واحد است، تجربیات ما نشان می‌دهد که کنترل‌کننده‌های متمرکز نه تنها مقیاس‌پذیری کافی را فراهم می‌کنند، بلکه ساده‌تر هستند و می‌توانند تصمیم‌گیری‌های با کیفیت‌تری انجام دهند.</p>
<p>**کنار گذاشتن کنترل‌کننده‌های غیرمتمرکز.** <br>
در این بخش، چند نمونه از معاوضه بین کنترل‌کننده‌های متمرکز و غیرمتمرکز را بررسی می‌کنیم. برای سوئیچ‌های شبکه در شبکه داخلی مرکز داده ما، اگرچه آن‌ها هنوز از BGP برای سازگاری استفاده می‌کنند، شبکه دارای یک کنترل‌کننده متمرکز است که می‌تواند مسیرهای مسیریابی را در هنگام ازدحام شبکه یا خرابی لینک‌ها بازنویسی کند.</p>
<p>به جز BGP، ما تقریباً تمام کنترل‌کننده‌های غیرمتمرکز را به کنترل‌کننده‌های متمرکز منتقل کرده‌ایم. به عنوان مثال، در شبکه WAN خصوصی ما، از RSVP-TE غیرمتمرکز به یک کنترل‌کننده متمرکز منتقل کردیم تا مسیرهای ترافیک ترجیحی را محاسبه کرده و مسیرهای پشتیبان را برای سناریوهای خرابی رایج به‌طور پیش‌گیرانه ایجاد کند. این امر منجر به استفاده کارآمدتر از منابع شبکه و همگرایی سریع‌تر در هنگام خرابی شبکه شده است.</p>
<p>برای ذخیره‌سازی کلید-مقدار، DHTها از مسیریابی چند‌هاپ (Multi-Hop) برای تعیین سرور مسئول یک کلید مشخص استفاده می‌کنند، در حالی که Cassandra از هش سازگار (Consistent Hashing) برای این منظور استفاده می‌کند. هر دو بدون نیاز به یک کنترل‌کننده مرکزی عمل می‌کنند. در مقابل، برای دستیابی به تعادل بار بهتر، چارچوب sharding ما از یک کنترل‌کننده مرکزی استفاده می‌کند تا shardهای حاوی کلید را به صورت پویا به سرورها اختصاص دهد.</p>
<p>برای توزیع داده‌های حجیم، ما از <strong>BitTorrent</strong> به <strong>Owl</strong> مهاجرت کردیم. این تغییر باعث شد که تصمیم‌گیری درباره‌ی اینکه هر همتا (peer) داده را از کجا دریافت کند، به‌صورت متمرکز انجام شود. نتیجه‌ی این کار افزایش قابل‌توجه سرعت دانلود بود.لازم به ذکر است که هم <strong>Owl</strong> و هم شبکه‌ی <strong>WAN خصوصی</strong> ما، <strong>بخش کنترلی (control plane)</strong> را به‌صورت متمرکز مدیریت می‌کنند تا تصمیم‌گیری‌ها بهینه‌تر شوند، اما همچنان از <strong>بخش داده‌ای (data plane) غیرمتمرکز</strong> برای ارسال و دریافت واقعی داده‌ها استفاده می‌کنند.</p>
<p>برای توزیع متادیتاهای کوچک (که در شکل ۴ توضیح بیشتری داده شده است)، در ابتدا از یک درخت توزیع سه‌سطحی پیاده‌سازی‌شده در جاوا استفاده کردیم. گره‌های میانی این درخت، سرورهای پروکسی اختصاصی بودند و گره‌های برگ، مشترک‌های برنامه(application subscribers)  بودند که می‌توانستند به صورت پویا به درخت اضافه یا از آن خارج شوند. هنگامی که این پیاده‌سازی دیگر نمی‌توانست مقیاس‌پذیری بیشتری داشته باشد، به یک درخت توزیع همتا به همتا (Peer-to-Peer) منتقل شدیم، جایی که گره‌های میانی نیز مشترک‌های برنامه بودند که داده‌ها را به سایر مشترک‌ها منتقل می‌کردند. با این حال، در میان میلیون‌ها مشترک برنامه، برخی از آن‌ها به دلیل ماهیت غیراختصاصی‌شان، اغلب با مشکلات عملکردی نویزی مواجه می‌شدند. در نتیجه، استفاده از آن‌ها به عنوان گره‌های میانی برای انتقال ترافیک، کمتر قابل اعتماد بود و منجر به اشکال‌زدایی مکرر و زمان‌بر می‌شد. در نهایت، پس از چند سال استفاده در محیط عملیاتی، درخت توزیع همتا به همتا را کنار گذاشتیم و به معماری اصلی که از سرورهای پروکسی اختصاصی استفاده می‌کرد، بازگشتیم. پیاده‌سازی اصلی جاوا را با یک پیاده‌سازی C++ با عملکرد بالاتر جایگزین کردیم، که به خوبی تا ده‌ها میلیون مشترک مقیاس‌پذیر بود.</p>
<p>**مطالعه موردی: شبکه سرویس مقیاس‌پذیر** <br>
در این بخش، از شبکه سرویس متا، ServiceRouter، به عنوان یک مطالعه موردی استفاده می‌کنیم تا طراحی سیستم‌های مقیاس‌پذیر را نشان دهیم و اثبات کنیم که کنترل‌کننده‌های متمرکز در ترکیب با صفحه داده غیرمتمرکز می‌توانند به خوبی در محیط مرکز داده مقیاس‌پذیر باشند. ServiceRouter میلیاردها درخواست RPC در ثانیه را بین میلیون‌ها روتر لایه ۷ (L7، یعنی لایه کاربردی) مسیریابی می‌کند.</p>
<p>شکل ۳ یک شبکه سرویس رایج در صنعت را نشان می‌دهد، جایی که هر فرآیند سرویس با یک پروکسی sidecar لایه ۷ همراه است که درخواست‌های RPC را برای سرویس مسیریابی می‌کند. به عنوان مثال، هنگامی که سرویس A روی سرور ۱ درخواست‌هایی به سرویس B ارسال می‌کند، پروکسی روی سرور ۱ آن‌ها را بین سرورهای ۲، ۳ و ۴ متعادل می‌کند. در حالی که این راه‌حل به طور گسترده مورد استفاده قرار می‌گیرد، برای زیرساخت‌های هایپرسکیل مقیاس‌پذیر نیست، زیرا کنترل‌کننده مرکزی نمی‌تواند به طور مستقیم جداول مسیریابی میلیون‌ها پروکسی sidecar را پیکربندی کند. کنترل‌کننده مرکزی دو وظیفه دارد: تولید متادیتای مسیریابی جهانی و مدیریت هر روتر لایه ۷. برای مقیاس‌پذیری، ما وظیفه اول را در کنترل‌کننده مرکزی نگه می‌داریم، اما وظیفه دوم را به روترهای لایه ۷ منتقل می‌کنیم، به طوری که هر روتر لایه ۷ بتواند خود را پیکربندی و مدیریت کند.</p>
<h2 id="مطالعه-موردی-شبکه-سرویس-مقیاسپذیر">مطالعه موردی: شبکه سرویس مقیاس‌پذیر<a hidden class="anchor" aria-hidden="true" href="#مطالعه-موردی-شبکه-سرویس-مقیاسپذیر">#</a></h2>
<p>در این بخش، از شبکه سرویس متا به نام <strong>ServiceRouter</strong> به عنوان یک مطالعه موردی استفاده می‌کنیم تا طراحی سیستم‌های مقیاس‌پذیر را نشان دهیم و ثابت کنیم که ترکیب کنترلرهای متمرکز با یک صفحه داده غیرمتمرکز می‌تواند در محیط‌های دیتاسنتر به خوبی مقیاس‌پذیری داشته باشد. ServiceRouter میلیاردها درخواست RPC (فراخوانی رویه‌ی راه دور) را در هر ثانیه بین میلیون‌ها روتر لایه ۷ (L7، یعنی لایه کاربردی) مسیریابی می‌کند.</p>
<p>شکل ۳ یک شبکه سرویس رایج در صنعت را نشان می‌دهد که در آن هر فرآیند سرویس با یک پروکسی جانبی (sidecar) L7 همراه است که درخواست‌های RPC را برای سرویس مسیریابی می‌کند. به عنوان مثال، وقتی سرویس A روی سرور ۱ درخواست‌هایی به سرویس B ارسال می‌کند، پروکسی روی سرور ۱ این درخواست‌ها را بین سرورهای ۲، ۳ و ۴ متعادل می‌کند. اگرچه این راه‌حل به طور گسترده استفاده می‌شود، اما برای زیرساخت‌های بسیار بزرگ (hyperscale) مقیاس‌پذیر نیست، زیرا کنترلر مرکزی نمی‌تواند به طور مستقیم جدول‌های مسیریابی میلیون‌ها پروکسی جانبی را پیکربندی کند. کنترلر مرکزی دو وظیفه دارد: تولید ابرداده‌های مسیریابی جهانی و مدیریت هر روتر L7. برای دستیابی به مقیاس‌پذیری، ما وظیفه اول را در کنترلر مرکزی نگه می‌داریم، اما وظیفه دوم را به روترهای L7 منتقل می‌کنیم و هر روتر L7 را خودپیکربند و خودمدیریت می‌کنیم.</p>
<figure>
    <img loading="lazy" src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig03.jpg"
         alt="My image"/> <figcaption>
            <p>شکل۳-شبکه‌ی سرویس (Service Mesh) مبتنی بر پروکسی سایدکار (Sidecar Proxy).</p>
        </figcaption>
</figure>

<p>شکل ۴ معماری مقیاس‌پذیر ServiceRouter را نشان می‌دهد. در بالای شکل، کنترلرهای مختلف به طور مستقل وظایف متفاوتی مانند ثبت سرویس‌ها، به‌روزرسانی تاخیرهای شبکه اندازه‌گیری شده و محاسبه جدول مسیریابی بین‌منطقه‌ای برای هر سرویس را انجام می‌دهند. هر کنترلر به طور مستقل پایگاه اطلاعات مسیریابی مرکزی (RIB) را به‌روزرسانی می‌کند و نیازی به پیکربندی یا مدیریت تک‌تک روترهای L7 ندارد. RIB یک پایگاه داده مبتنی بر Paxos است و می‌تواند از طریق تقسیم‌بندی (sharding) مقیاس‌پذیری داشته باشد. با کمک RIB، کنترلرها بدون حالت (stateless) می‌شوند و به راحتی از طریق تقسیم‌بندی مقیاس‌پذیری پیدا می‌کنند. به عنوان مثال، چندین نمونه کنترلر می‌توانند به طور همزمان جدول‌های مسیریابی بین‌منطقه‌ای را برای سرویس‌های مختلف محاسبه کنند.</p>
<figure>
    <img loading="lazy" src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig04.jpg"
         alt="My image"/> <figcaption>
            <p>شکل۴- معماری سطح بالا‌ی ServiceRouter</p>
        </figcaption>
</figure>

<p>در وسط شکل ۴، لایه توزیع از هزاران نسخه تکثیرشده RIB استفاده می‌کند تا ترافیک خواندن از میلیون‌ها روتر L7 را مدیریت کند. در پایین شکل، هر روتر L7 با هدایت RIB، بدون نیاز به دخالت مستقیم صفحه کنترل، خود را پیکربندی می‌کند. روترهای L7 ناهمگون (heterogeneous) پشتیبانی می‌شوند که می‌توانند شامل متعادل‌کننده‌های بار (load balancers)، سرویس‌هایی با کتابخانه‌های مسیریابی توکار یا پروکسی‌های جانبی باشند.</p>
<p>با توجه به ServiceRouter، می‌توانیم با استفاده از تکنیک‌هایی مانند کنترل‌کننده‌های بدون حالت (stateless)، تکه‌تکه کردن (sharding) کنترل‌کننده‌ها و حذف عملکردهای غیرضروری از کنترل‌کننده‌های مرکزی – مثل مدیریت جداگانه‌ی مسیریاب‌های لایه ۷ – به مقیاس‌پذیری خوبی با کنترل‌کننده‌های متمرکز دست پیدا کنیم.</p>
<h1 id="جهتگیریهای-آینده">جهت‌گیری‌های آینده<a hidden class="anchor" aria-hidden="true" href="#جهتگیریهای-آینده">#</a></h1>
<p>علیرغم پیچیدگی زیرساخت‌های بسیار بزرگ (hyperscale) متا، در اینجا یک مرور کلی و مختصر ارائه کردیم و بر بینش‌های کلیدی حاصل از توسعه آن تأکید نمودیم. در پایان، دیدگاه‌های خود را درباره روندهای احتمالی آینده برای زیرساخت‌های بسیار بزرگ به اشتراک می‌گذاریم.</p>
<p><strong>هوش مصنوعی (AI):</strong> <br>
بارکاری هوش مصنوعی به بزرگ‌ترین دسته‌ی بارکاری در مراکز داده تبدیل شده‌اند. پیش‌بینی می‌کنیم که تا پایان این دهه، بیش از نیمی از انرژی دیتاسنترها به بارکاری‌های هوش مصنوعی اختصاص یابد. به دلیل ویژگی‌های متمایز هوش مصنوعی، مانند نیاز به منابع بیشتر و شبکه‌های با پهنای باند بالاتر، انتظار داریم که هوش مصنوعی هر جنبه‌ای از زیرساخت را به طور عمیقی متحول کند. در دو دهه گذشته، زیرساخت‌های بسیار بزرگ عمدتاً با استفاده از رویکرد مقیاس‌افقی (scaling-out) و بهره‌گیری از تعداد زیادی سرور کم‌هزینه موفق شده‌اند. با این حال، خوشه‌های آینده هوش مصنوعی به احتمال زیاد از رویکرد مقیاس‌عمودی (scale-up) استفاده خواهند کرد که در ابررایانه‌های گذشته دیده می‌شد، مانند استفاده از دسترسی مستقیم به حافظه از راه دور (RDMA) روی اترنت برای ارائه شبکه‌ای با پهنای باند بالا و تاخیر کم که برای آموزش مدل‌های یادگیری ماشین در مقیاس بزرگ ضروری است. رویکرد متا به هوش مصنوعی با طراحی همزمان تمام اجزای سیستم، از PyTorch تا مدل‌های یادگیری ماشین، تراشه‌های هوش مصنوعی، شبکه‌ها، دیتاسنترها، سرورها، ذخیره‌سازی، برق و خنک‌کننده متمایز می‌شود.</p>
<p><strong>سخت‌افزارهای خاص حوزه (Domain-specific hardware):</strong> <br>
برخلاف روند کاهش تنوع سخت‌افزاری در دهه ۲۰۰۰، پیش‌بینی می‌کنیم که شاهد گسترش سخت‌افزارهای سفارشی و تخصصی برای اهداف مختلف باشیم، مانند آموزش و استنتاج هوش مصنوعی، مجازی‌سازی، رمزگذاری ویدیو، رمزنگاری، فشرده‌سازی، حافظه‌های لایه‌بندی شده و همچنین پردازش درون شبکه و درون ذخیره‌سازی. دلیل این امر آن است مقیاس‌پذیری اقتصادی به شرکت‌های ابرمقیاس این امکان را می‌دهد که سخت‌افزارهای تخصصی را در ابعاد وسیع طراحی و مستقر کنند و هزینه‌ها را کاهش دهند. در نتیجه، این امر فرصت‌های جدیدی را برای پشته نرم‌افزاری فراهم می‌کند تا از یک ناوگان متنوع و پیشرفته به شکلی بهینه بهره‌برداری کند.</p>
<p><strong>دیتاسنترهای لبه (Edge datacenters):</strong> <br>
انتظار داریم که استفاده از برنامه‌های متاورس و اینترنت اشیا (IoT) به طور قابل توجهی افزایش یابند. به عنوان مثال، گیمینگ ابری (cloud gaming) رندرینگ گرافیکی را از دستگاه‌های کاربر به سرورهای GPU در دیتاسنترهای لبه منتقل می‌کند و نیازمند تاخیر شبکه کمتر از ۲۵ میلی‌ثانیه است. تقاضا برای پاسخگویی بلادرنگ احتمالاً باعث رشد قابل توجهی در تعداد و اندازه دیتاسنترهای لبه خواهد شد. در نتیجه، صفحه کنترل زیرساخت باید به گونه‌ای تطبیق یابد که بتواند ناوگانی پراکنده‌تر را مدیریت کند، ترجیحاً با بهبود Global-DaaC تا پیچیدگی زیرساخت پراکنده را از دید توسعه‌دهندگان برنامه‌ها پنهان کند.</p>
<p><strong>بهره‌وری توسعه‌دهندگان:</strong> <br>
در دو دهه گذشته، ابزارهای اتوماسیون به طور قابل توجهی بهره‌وری مدیران سیستم را افزایش داده‌اند و منجر به نسبت سرور به مدیر بسیار بالاتری شده‌اند. در مقابل، توسعه نرم‌افزارهای عمومی همچنان پرزحمت است و رشد بهره‌وری در این حوزه کندتر بوده است. در این دهه، انتظار داریم که این روند تغییر کند و بهره‌وری توسعه‌دهندگان به دو دلیل به سرعت افزایش یابد: تولید و اشکال‌زدایی کد با کمک هوش مصنوعی، و پارادایم‌های برنامه‌نویسی سرورلس (serverless) کاملاً یکپارچه در زمینه‌های تخصصی. FrontFaaS متا نمونه‌ای از مورد دوم است، و پیش‌بینی می‌کنیم که پارادایم‌های برنامه‌نویسی بسیار مولد برای زمینه‌های تخصصی بیشتری ظهور کنند.</p>
<p>ما پیش‌بینی می‌کنیم که نوآوری سریع در زیرساخت‌های هایپرسکیل که در دو دهه‌ی گذشته شاهد آن بوده‌ایم، در دهه‌ی آینده نیز ادامه یابد، به‌ویژه با پیشرفت‌های هوش مصنوعی. ما ابرشرکت‌ها را تشویق می‌کنیم که دیدگاه‌های خود را به اشتراک بگذارند تا جامعه بتواند به‌صورت جمعی روند پیشرفت را تسریع کند.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://mhhio.github.io/posts/threads-synchronization-problem/">
    <span class="title">صفحه بعدی »</span>
    <br>
    <span>هماهنگ‌سازی thread ها: یک سوال به ظاهر ساده!</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on twitter"
        href="https://twitter.com/intent/tweet/?text=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7&amp;url=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f&amp;title=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7&amp;summary=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7&amp;source=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f&title=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on whatsapp"
        href="https://api.whatsapp.com/send?text=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7%20-%20https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on telegram"
        href="https://telegram.me/share/url?text=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7&amp;url=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share بررسی زیرساخت ابرمقیاس متا on ycombinator"
        href="https://news.ycombinator.com/submitlink?t=%d8%a8%d8%b1%d8%b1%d8%b3%db%8c%20%d8%b2%db%8c%d8%b1%d8%b3%d8%a7%d8%ae%d8%aa%20%d8%a7%d8%a8%d8%b1%d9%85%d9%82%db%8c%d8%a7%d8%b3%20%d9%85%d8%aa%d8%a7&u=https%3a%2f%2fmhhio.github.io%2fposts%2fmetas-hyperscale-infrastructure-overview-and-insights%2f">
        <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
            xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
            <path
                d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://mhhio.github.io/">mhhio&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'کپی';

        function copyingDone() {
            copybutton.innerHTML = 'کپی شد!';
            setTimeout(() => {
                copybutton.innerHTML = 'کپی';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
